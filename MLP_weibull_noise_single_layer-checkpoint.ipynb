{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 介绍 : 随机结点加weibull分布noise   not bp   单个样本循环\n",
    "# 可调整的参数 : \n",
    "#     Sig : 调整高斯noise的标准差\n",
    "#     Sigm : weibull分布参数,get_grad参数\n",
    "#     learning rate\n",
    "\n",
    "# 未解决的问题:\n",
    "#         loss降不下去\n",
    "#         append 的 loss不对  ok\n",
    "#         batchsize不能为1    ok\n",
    "#         参数没调好\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kzw\\AppData\\Local\\conda\\conda\\envs\\python27\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Python imports\n",
    "import numpy as np # Matrix and vector computation package\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib inline\n",
    "# Set the seed of the numpy random number generator so that the tutorial is reproducable\n",
    "np.random.seed(seed=1)\n",
    "from sklearn import datasets, cross_validation, metrics # data and evaluation utils\n",
    "from matplotlib.colors import colorConverter, ListedColormap # some plotting functions\n",
    "import itertools\n",
    "import collections\n",
    "from scipy import stats\n",
    "from scipy.stats import weibull_min\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#给逻辑层的输出的头部插入1\n",
    "def array_add_data(activ):\n",
    "    \"\"\"\n",
    "    参数activ : 每层的输出(激活)  第一层 : (32L, 20L)\n",
    "    作用 : 在数组头部插入数值1\n",
    "    \"\"\"\n",
    "    r = []\n",
    "    ret1 = 0\n",
    "    for i in range(activ.shape[0]):\n",
    "        ret1 = np.array(np.append([1],activ[i])) #list\n",
    "        r.append(ret1)\n",
    "    r = np.array(r) # 类型转换:list -> array\n",
    "    return r\n",
    "\n",
    "#给所有样本头部插入1\n",
    "def array_add_data2(activ):\n",
    "    \"\"\"\n",
    "        参数activ : 样本\n",
    "        作用 : 在数组头部插入数值1\n",
    "    \"\"\"\n",
    "    r = []\n",
    "    ret1 = np.append([1],activ)\n",
    "    r.append(ret1)\n",
    "    ret2 = np.array(r)\n",
    "    return ret2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the non-linear functions used\n",
    "def logistic(z): \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_deriv(y):  # Derivative of logistic function\n",
    "    return np.multiply(y, (1 - y))\n",
    "    \n",
    "def softmax(z): \n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layers used in this model\n",
    "class Layer(object):\n",
    "    \n",
    "    def get_params_iter(self):\n",
    "        return []\n",
    "    \n",
    "    def get_params_grad(self, X, output_grad):\n",
    "        return []\n",
    "      \n",
    "    def get_params_update(self, X, loss):\n",
    "        return []\n",
    "    \n",
    "    def add_noise(self):\n",
    "        pass\n",
    "    \n",
    "    def remove_noise(self):\n",
    "        pass\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        pass\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad=None, T=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class discreteLayer(Layer):\n",
    "    \"\"\"The linear layer performs a linear transformation to its input.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_in, n_out,sig,batch_size):\n",
    "        \"\"\" \n",
    "            对隐藏层参数进行初始化\n",
    "            n_in是输入变量的数量\n",
    "            n_out是输出变量的数量\n",
    "        \"\"\"\n",
    "        self.sig=sig\n",
    "        self.batch_size = batch_size\n",
    "        self.node_sum = n_out\n",
    "        self.W = np.random.randn(n_in, n_out) * 0.1\n",
    "        self.W[0:1,:] = 0\n",
    "        self.noise = 0\n",
    "    # training set 训练时: 添加高斯noise\n",
    "    def add_noise(self): \n",
    "        self.noise = np.random.randn(self.batch_size,self.node_sum)*self.sig\n",
    "#         print(\"--- noise ---\")\n",
    "#         print(type(self.noise))\n",
    "#         print(self.noise.shape)\n",
    "#         print(self.noise)\n",
    "    \n",
    "    # 移除选定结点的高斯noise\n",
    "    def remove_targetnode_gaosi_noise(self,node_index):\n",
    "#         i = 0\n",
    "        for n in self.noise:\n",
    "#             print(\"n[node_index - 1].type\" + str(type(n[node_index - 1])))\n",
    "            n[node_index - 1] = 0 \n",
    "    \n",
    "    # 移除高斯noise   (validation set 验证时)\n",
    "    def remove_noise(self):\n",
    "        self.noise = np.zeros(self.W.shape[1], dtype=float, order='C')\n",
    "    \n",
    "    def get_params_iter(self):\n",
    "        return self.W\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"\n",
    "            返回线性层的输出\n",
    "        \"\"\"\n",
    "#         print(\"--- linear out ---\")\n",
    "#         print(self.noise)\n",
    "        xw = X.dot(self.W)\n",
    "        re = xw + self.noise \n",
    "        \n",
    "#         print(type(xw))\n",
    "#         print(type(self.noise))\n",
    "#         print(xw.shape)\n",
    "#         print(self.noise.shape)\n",
    "        return re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticLayer(Layer):\n",
    "    \"\"\"The logistic layer applies the logistic function to its inputs.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.noise = 0\n",
    "        self.b = 0\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        # 在头部插入数值1\n",
    "        logi = logistic(X)\n",
    "        Y = array_add_data(logi) #第一层X,shape: (32L, 20L)\n",
    "#         print(\"--- logistic out ---\")\n",
    "#         print(type(Y))\n",
    "#         print(Y.shape)\n",
    "#         print(Y)\n",
    "        return Y\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        return np.multiply(logistic_deriv(Y), output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxOutputLayer(Layer):\n",
    "    \"\"\" \n",
    "        输出层逻辑函数\n",
    "    \"\"\"  \n",
    "    def __init__(self):\n",
    "        self.noise = 0\n",
    "        self.b = 0\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return softmax(X)\n",
    "    \n",
    "#     def get_input_grad(self, Y, T):\n",
    "#         \"\"\"Return the gradient at the inputs of this layer.\n",
    "#             Y:最后层激活\n",
    "#             T:target\n",
    "#         \"\"\"\n",
    "#         return (Y - T) / Y.shape[0]   # /Y.shape[0] -->取平均值(输入的所有样本)\n",
    "    \n",
    "    def get_cost(self, A, T):\n",
    "        \"\"\"Return the cost at the output of this output layer.\"\"\"\n",
    "        re = - np.multiply(T, np.log(A)).sum() / A.shape[0]\n",
    "        return re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weibull(k,lambd,batch_size):\n",
    "    \"\"\"\n",
    "        从weibull分布中随机取32个数(一个batch)\n",
    "        return array\n",
    "    \"\"\"\n",
    "    weibull_array = weibull_min.rvs(k, loc=0, scale=lambd, size=1000)\n",
    "    d = np.array(random.sample(weibull_array, batch_size))\n",
    "    return d\n",
    "\n",
    "# sigm = 1 # 可调整\n",
    "# k = 2 # 形状参数            \n",
    "# lambd = math.sqrt(2) * sigm # 比例参数\n",
    "# wei=get_weibull(k,lambd,1)\n",
    "# print(type(wei))\n",
    "# print(wei.shape)\n",
    "# print(wei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weibull_noise(Y,weibull_noise,node_index):\n",
    "    # Y : 线性变换的输出\n",
    "    # 给选定的结点加weibullnoise  一个batch中32个样本加不同的weibull noise\n",
    "#     print(\"----- weibull -----\")\n",
    "#     print(Y.shape)\n",
    "#     print(node_index - 1)\n",
    "#     print(weibull_noise)\n",
    "#     print(Y[0])\n",
    "    for i in range(Y.shape[0]): #32\n",
    "        Y[i][node_index - 1] = Y[i][node_index - 1] + weibull_noise[i]\n",
    "#     print(Y[0])\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 验证weibull函数的正确性\n",
    "# from scipy import stats \n",
    "# import scipy\n",
    "# import seaborn as sns \n",
    "# from scipy.special import gamma\n",
    "# # weibull正确性函数验证\n",
    "# k = 1\n",
    "# lambd = 1\n",
    "# data = weibull_min.rvs(k, loc=0, scale=lambd, size=10000)\n",
    "# # print(type(data)) # array\n",
    "# data2 = sorted(data)\n",
    "# # print(data2)\n",
    "# #1.密度函数图验证 : 不确定\n",
    "# # sns.distplot(data2, rug=True, hist=True)\n",
    "# # plt.hist(data2,bins=100,normed=1,edgecolor='black',cumulative=False)\n",
    "# # plt.show()\n",
    "# #2.方差验证  ok  相差0.02 - 0.05\n",
    "# varian_cur = np.var(data2)\n",
    "# varian_theory = lambd **2 * (  gamma(1 + 2/k) - gamma(1 + 1/k)**2  )\n",
    "# print(\"----方差-----\")\n",
    "# print(varian_cur)\n",
    "# print(varian_theory)\n",
    "# #3.均值验证 ok\n",
    "# print(\"----均值-----\")\n",
    "# mean_cur = np.mean(data2)\n",
    "# mean_theory = lambd * gamma(1 + 1/k)\n",
    "# print(mean_cur)\n",
    "# print(mean_theory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_node_index():\n",
    "#     \"\"\"\n",
    "#         得到随机结点的index\n",
    "#     \"\"\"\n",
    "#     # 得到第几层\n",
    "#     layer_index = random.choice([1,2,3])\n",
    "#     # 得到第几个结点\n",
    "#     if (layer_index == 1) or (layer_index == 2):\n",
    "#         node_index = random.choice([1,2,3,4,5,6,7,8,9,10,12,13,14,15,16,17,18,19,20])\n",
    "#     else:\n",
    "#         node_index = random.choice([1,2,3,4,5,6,7,8,9,10])\n",
    "#     #将结点与逻辑层对应\n",
    "#     if layer_index == 2:\n",
    "#         layer_index = 3\n",
    "#     elif layer_index == 3:\n",
    "#         layer_index = 5\n",
    "#     return layer_index,node_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forward_noise_step0(input_samples, layers):\n",
    "#     \"\"\"\n",
    "#         validation set 验证时的前向传播\n",
    "#         条件 : 去除所有的noise\n",
    "#     \"\"\"\n",
    "#     activations = [input_samples] \n",
    "#     X = input_samples\n",
    "#     for layer in layers:\n",
    "#         #移除高斯noise\n",
    "#         layer.remove_noise()\n",
    "#         Y = layer.get_output(X) \n",
    "#         activations.append(Y) \n",
    "#         X = activations[-1]  \n",
    "#     return activations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练时的前向传播 \n",
    "def forward_noise_step(input_samples, layers,k,lambd,batch_size,par_layer_index,par_node_index):\n",
    "    \"\"\"\n",
    "        return:各层的输出,随机结点的index\n",
    "    \"\"\"\n",
    "    # 得到随机结点的index\n",
    "    layer_index,node_index = par_layer_index,par_node_index\n",
    "    # get weibull noise\n",
    "    weibull_noise1 = get_weibull(k,lambd,batch_size) #正向  shape:(1L,)  type:array\n",
    "    weibull_noise2 = weibull_noise1 * (-1)#反向\n",
    "\n",
    "    # get正向weibull的activation\n",
    "    activations1 = [input_samples] #  input_samples.shape : (1L, 65L)\n",
    "    X = input_samples\n",
    "    for index in range(len(layers)): # 0,1,2,3\n",
    "        index_target1 = index + 1 # 指定当前在第几层\n",
    "        layer = layers[index]\n",
    "        # 给选定结点加weibull noise && 移除高斯noise\n",
    "        if index_target1 == layer_index: \n",
    "            # 给所有结点加高斯noise\n",
    "            layer.add_noise()   # 每个样本加不同的高斯noise\n",
    "#             print(\"----- target -----\")\n",
    "#             print(layer.noise)\n",
    "            # 移除指定结点的高斯noise\n",
    "            layer.remove_targetnode_gaosi_noise(node_index)\n",
    "#             print(layer.noise)\n",
    "            Y = layer.get_output(X)  # z=xw+b一维数组 \n",
    "            # 选定结点加weibull_noise\n",
    "            Y = add_weibull_noise(Y,weibull_noise1,node_index)\n",
    "        else:\n",
    "#             print(index + 1)\n",
    "#             print(layer)\n",
    "            layer.add_noise()\n",
    "            Y = layer.get_output(X)\n",
    "#             print(type(Y))\n",
    "#             print(Y.shape)\n",
    "#             print(Y)\n",
    "        activations1.append(Y)  # 存储输出以供将来处理\n",
    "        X = activations1[-1]  # 将前一层的输出设置为当前层的激活\n",
    "    \n",
    "    # get反向weibull的activation\n",
    "    activations2 = [input_samples] #  input_samples.shape : (32L, 65L)\n",
    "    X = input_samples\n",
    "    for index in range(len(layers)):\n",
    "        index_target1 = index + 1 # 指定当前在第几层\n",
    "        layer = layers[index]\n",
    "        # 给选定的结点加weibull noise && 移除高斯noise\n",
    "        if index_target1 == layer_index: \n",
    "            layer.add_noise()\n",
    "            # 移除指定结点的高斯noise\n",
    "            layer.remove_targetnode_gaosi_noise(node_index)\n",
    "            Y = layer.get_output(X)  # z=xw+b都是一维数组     type(Y):array\n",
    "            # 选定结点加weibull_noise\n",
    "            Y = add_weibull_noise(Y,weibull_noise2,node_index)\n",
    "        else:\n",
    "            layer.add_noise()\n",
    "            Y = layer.get_output(X)\n",
    "        activations2.append(Y) \n",
    "        X = activations2[-1] \n",
    "#     print(\"----------------- 一个forward  ------------------\")\n",
    "    return activations1,activations2,layer_index,node_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_grad(ac,layers,loss,c,layer_index):\n",
    "    \"\"\"\n",
    "        获得选定结点w的grad(求一个batch的平均)\n",
    "    \"\"\"\n",
    "    # 选定要update的层\n",
    "    layer = layers[layer_index - 1]\n",
    "    # 获得层对应的输入\n",
    "    x = ac[layer_index - 1]\n",
    "    # 得到grad\n",
    "#     grad = np.zeros(x.shape[1], dtype=float, order='C')\n",
    "#     for i in range(len(x)):\n",
    "#         g = (1)*c*x[i]*(loss)\n",
    "#         grad += g\n",
    "#     grad = np.sum(x,axis=0)*c*loss/x.shape[0]\n",
    "    grad = c*x*(loss)\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params2(layers,layer_index,grad,node_index,learning_rate):\n",
    "    \"\"\"\n",
    "        更新选定节点的权重\n",
    "    \"\"\"\n",
    "    # 选定要update的层\n",
    "    layer = layers[layer_index - 1]  \n",
    "    # 获得对应层的权重w\n",
    "    w = layer.get_params_iter()\n",
    "    # 更新\n",
    "    w[:,node_index - 1] -= learning_rate * grad\n",
    "    layer.W = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#得到样本\n",
    "\n",
    "# 在所有样本的头部添加数值1\n",
    "digits = datasets.load_digits()\n",
    "data = digits.data # array\n",
    "# print(data.shape) # (1797L, 64L)\n",
    "target_list = [] # list\n",
    "target_data = []\n",
    "# single_array = np.array([1])\n",
    "for i in range(len(data)):\n",
    "    # 取出数据\n",
    "    a = data[i]\n",
    "    # 头部添加1 \n",
    "    a_temp = array_add_data2(a)[0]\n",
    "    # 添加到总的array中\n",
    "    target_list.append(a_temp)\n",
    "target_data = np.array(target_list) \n",
    "\n",
    "T = np.zeros((digits.target.shape[0],10))\n",
    "T[np.arange(len(T)), digits.target] += 1\n",
    "\n",
    "X_train, X_test, T_train, T_test = cross_validation.train_test_split(\n",
    "    target_data, T, test_size=0.4)\n",
    "X_validation, X_test, T_validation, T_test = cross_validation.train_test_split(\n",
    "    X_test, T_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 创建minibatches\n",
    "# X_train = X_train[0:1024,:] #取1024方便计算\n",
    "# T_train = T_train[0:1024,:]\n",
    "batch_size = 1#    adjust\n",
    "# nb_of_batches = X_train.shape[0] / batch_size  # 32批\n",
    "# # Create batches (X,Y) from the training set\n",
    "# XT_batches = zip(\n",
    "#     np.array_split(X_train, nb_of_batches, axis=0),  \n",
    "#     np.array_split(T_train, nb_of_batches, axis=0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "hidden_neurons_1 = 20  # 第一个隐藏层的神经元数目\n",
    "# hidden_neurons_2 = 20  # 第二隐层神经元数目\n",
    "# Create the model\n",
    "sigm = 1        # adjust\n",
    "layers = [] #\n",
    "# 添加第一个隐层\n",
    "layers.append(discreteLayer(X_train.shape[1], hidden_neurons_1,sigm,batch_size))   # shape(65,20) nodesum = 20\n",
    "layers.append(LogisticLayer())\n",
    "# # 添加第二个隐层\n",
    "# layers.append(discreteLayer(21, hidden_neurons_2,sigm,batch_size))# shape(20,20) -> (21,20): 因为上一层的激活增加一个数\n",
    "# layers.append(LogisticLayer())\n",
    "# 添加输出层\n",
    "layers.append(discreteLayer(21, T_train.shape[1],sigm,batch_size)) # w.shape(20,10) -> (21,100): 因为上一层的激活增加一个数\n",
    "layers.append(SoftmaxOutputLayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "均值 : -0.09179162601423035\n",
      "标准误差 : 0.000227084980021\n"
     ]
    }
   ],
   "source": [
    "#迭代训练 : 单张图片  单隐藏层 指定结点\n",
    "\n",
    "# Perform backpropagation for noise \n",
    "# 定义超参数\n",
    "# learning_rate = 0.1  # adjust\n",
    "# weibull分布需要的参数\n",
    "k = 2 # 形状参数            \n",
    "lambd = (math.sqrt(2)) * sigm # 比例参数\n",
    "# get grad需要的参数c\n",
    "c = 1/(math.sqrt(2*math.pi*sigm*sigm))    # 两种都试下\n",
    "\n",
    "# 成本\n",
    "# minibatch_costs = []\n",
    "# training_costs = []\n",
    "# validation_costs = []\n",
    "\n",
    "x1 = target_data[0]\n",
    "t1 = T[0]\n",
    "x1 = np.expand_dims(x1, axis=0)\n",
    "# print(type(x1))\n",
    "# print(x1.shape)\n",
    "# print(x1)\n",
    "# print(\"------------------------------------------------------\")\n",
    "# print(type(t1))\n",
    "# print(t1.shape)\n",
    "# print(t1)\n",
    "\n",
    "# 开始训练\n",
    "max_nb_of_iterations = 50000 # 训练次数\n",
    "# 选定一个结点\n",
    "layer_index,node_index,weight_index = 3,1,1 #layer : 1,3\n",
    "w1 = []\n",
    "for iteration in range(max_nb_of_iterations): \n",
    "    loss1 = 0\n",
    "    loss2 = 0\n",
    "    activations1,activations2,layer_index,node_index = forward_noise_step(x1, layers,k,lambd,batch_size,layer_index,node_index)\n",
    "    loss1 = layers[-1].get_cost(activations1[-1][0], t1) #activations1[-1]二维数组: [[.....]]\n",
    "    loss2 = layers[-1].get_cost(activations2[-1][0], t1) \n",
    "    loss = loss1 - loss2\n",
    "    grad = get_param_grad(activations1,layers,loss,c,layer_index) #二维数组[[...]]\n",
    "    # 显示循环次数\n",
    "    if (iteration + 1) % 10000 == 0:\n",
    "        print(iteration + 1) \n",
    "    w1.append(grad[0][weight_index])\n",
    "print('均值 : '+ str(np.mean(w1)))\n",
    "print('标准误差 : '+ \"{:.15f}\".format( np.std(w1)/math.sqrt(max_nb_of_iterations) ))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1L, 65L)\n",
      "<type 'numpy.ndarray'>\n",
      "[[ 1.  0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.\n",
      "   3. 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.\n",
      "   0.  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10.\n",
      "  12.  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]]\n",
      "---------------------------------------------------------------\n",
      "(10L,)\n",
      "<type 'numpy.ndarray'>\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(x1.shape)\n",
    "print(type(x1))\n",
    "print(x1)\n",
    "print(\"---------------------------------------------------------------\")\n",
    "print(t1.shape)\n",
    "print(type(t1))\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-0.1619839340666729   -0.1615295308501942  -0.16265979174936893   -0.16122839945027728   -0.16111464860923713\n",
    " 0.000669730900059      0.000669730900059    0.000680700473762     0.000677064762774      0.000672577088526"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "----------\n",
    "weibull single_layer : sigma = 1   index: 3,1,1   基本都在置信区间内 个别的不在置信区间内\n",
    "-0.9187890667783043  -0.9153729136579379    -0.9164419739874899    -0.9175245232067069   -0.9163145391676504   -0.9144701511486839*\n",
    " 0.001735109224339    0.001729150496717      0.001720759388408      0.001735231569326     0.001726586746013     0.001731652960506\n",
    "    \n",
    "-0.9165002683277187   -0.9180347577495181   -0.9162900548383293    -0.9165897248180369   -0.9171871662136265   -0.9145976113438883*\n",
    "0.001735137133986      0.001732552461784      0.001733514169460      0.001728139277176     0.001728498368857     0.001731011503245\n",
    "\n",
    "=========================================================================================================================================\n",
    "\n",
    "BP算法 :(梯度的数值逼近计算grad) : idnex:2,1,1    sigma = 1  iteration=50000   eps = 0.000000001  基本都在置信区间内       \n",
    "mean : 0.010026589128686593  0.009918825861765512   0.009983802152502273   0.010113635804803735  0.00994806789736402   0.009963361458364961\n",
    "SE   : 0.000073842244197     0.000070438961814      0.000072343052882      0.000072879454182     0.000071686364345     0.000070908224526\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.16085979995\n",
      "-0.161787225698\n"
     ]
    }
   ],
   "source": [
    "# 验证置信区间\n",
    "d1 = -0.1615295308501942\n",
    "eps1 = 0.000669730900059\n",
    "d2 = -0.16111464860923713\n",
    "eps2 = 0.000672577088526\n",
    "print(d1 + eps1) #大\n",
    "print(d2 - eps2) #小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Perform backpropagation for noise \n",
    "# # 定义超参数\n",
    "# learning_rate = 0.1   # adjust  0.1 0.01 0.001 0.0001 0.00001\n",
    "# # weibull分布需要的参数\n",
    "# sigm = 10         # adjust  10 1 0.1 0.01 0.001 0.0001 0.00001 0.000001\n",
    "# k = 2 # 形状参数            \n",
    "# lambd = (math.sqrt(2)) * sigm # 比例参数\n",
    "# # get_grad需要的参数c\n",
    "# c = -1*(1/(math.sqrt(2*math.pi*sigm*sigm)))\n",
    "\n",
    "# # 定义模型\n",
    "# hidden_neurons_1 = 20  # 第一个隐藏层的神经元数目\n",
    "# hidden_neurons_2 = 20  # 第二隐层神经元数目\n",
    "# # Create the model\n",
    "# layers = [] #\n",
    "# # 添加第一个隐层\n",
    "# layers.append(discreteLayer(X_train.shape[1], hidden_neurons_1,sigm,batch_size))   # shape(65,20) nodesum = 20\n",
    "# layers.append(LogisticLayer())\n",
    "# # 添加第二个隐层\n",
    "# layers.append(discreteLayer(21, hidden_neurons_2,sigm,batch_size))# shape(20,20) -> (21,20): 因为上一层的激活增加一个数\n",
    "# layers.append(LogisticLayer())\n",
    "# # 添加输出层\n",
    "# layers.append(discreteLayer(21, T_train.shape[1],sigm,batch_size)) # shape(20,10) -> (21,100): 因为上一层的激活增加一个数\n",
    "# layers.append(SoftmaxOutputLayer())\n",
    "# # 成本\n",
    "# minibatch_costs = []\n",
    "# training_costs = []\n",
    "# validation_costs = []\n",
    "\n",
    "# # 开始训练\n",
    "# fault_times = 0 # 计算方差大于均值的次数\n",
    "# max_nb_of_iterations = 50000 # 训练次数\n",
    "# for iteration in range(max_nb_of_iterations): \n",
    "#     loss1 = 0\n",
    "#     loss2 = 0\n",
    "#     for x, t in XT_batches:  \n",
    "#         # 得到正负weibull的激活,选定结点的index\n",
    "#         activations1,activations2,layer_index,node_index = forward_noise_step(x, layers,k,lambd,batch_size)\n",
    "#         # 得到正负loss\n",
    "#         loss1 = layers[-1].get_cost(activations1[-1], t) # loss正\n",
    "#         loss2 = layers[-1].get_cost(activations2[-1], t) # loss负\n",
    "#         # 得到loss\n",
    "#         loss = loss1 - loss2\n",
    "#         # 获得梯度grad\n",
    "#         grad = get_param_grad(activations1,layers,loss,c,layer_index)\n",
    "#         # 更新选定结点的权重w\n",
    "#         update_params2(layers,layer_index,grad,node_index,learning_rate)\n",
    "        \n",
    "#         # 计算均值大于标准的次数\n",
    "#         if abs(np.mean(grad)) < abs(np.std(grad)):\n",
    "#             fault_times += 1\n",
    "#         # 显示grad的均值,方差\n",
    "# #         print(type(grad))\n",
    "# #         print(grad.shape)\n",
    "#         print('均值 :  '+ str(np.mean(grad)))\n",
    "#         print('标准差 : '+ \"{:.8f}\".format(np.std(grad)))\n",
    "#         print(\"=============================================\")\n",
    "\n",
    "#         # Get minibatch cost\n",
    "#         activations3 = forward_noise_step0(x, layers)\n",
    "#         minibatch_cost = layers[-1].get_cost(activations3[-1], t)\n",
    "#         minibatch_costs.append(minibatch_cost)\n",
    "\n",
    "#     # Get full training cost\n",
    "#     activations4 = forward_noise_step0(X_train, layers)\n",
    "#     train_cost = layers[-1].get_cost(activations4[-1], T_train)\n",
    "#     training_costs.append(train_cost)\n",
    "#     # Get full validation cost\n",
    "#     activations5 = forward_noise_step0(X_validation, layers)\n",
    "#     validation_cost = layers[-1].get_cost(activations5[-1], T_validation)\n",
    "#     validation_costs.append(validation_cost)\n",
    "#     #if len(validation_costs) > 3:\n",
    "#         # Stop training if the cost on the validation set doesn't decrease\n",
    "#         #  for 3 iterations***********************************\n",
    "#         #if validation_costs[-1] >= validation_costs[-2] >= validation_costs[-3]:\n",
    "#          #   break\n",
    "#     # 显示迭代次数\n",
    "#     if (iteration + 1) % 1000 == 0:\n",
    "#         print(iteration + 1) \n",
    "# #显示方差大于均值的次数\n",
    "# print(\"标准差(绝对值)大于均值(绝对值)的次数为 : \" + str(fault_times))\n",
    "# nb_of_iterations = iteration + 1  # The number of iterations that have been executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Plot the minibatch, full training set, and validation costs\n",
    "# minibatch_x_inds = np.linspace(1, max_nb_of_iterations*nb_of_batches, num=max_nb_of_iterations*nb_of_batches, endpoint=True)\n",
    "# iteration_x_inds = np.linspace(1, max_nb_of_iterations, num=max_nb_of_iterations, endpoint=True)\n",
    "# # minibatch_x_inds = np.linspace(1, nb_of_iterations, num=nb_of_iterations, endpoint=True)\n",
    "# # iteration_x_inds = np.linspace(1, nb_of_iterations, num=nb_of_iterations, endpoint=True)\n",
    "\n",
    "# # 计算loss的variance和mean\n",
    "# # minibatch\n",
    "# # print(minibatch_costs)\n",
    "# print('minibatch均值 : '+ str(np.mean(minibatch_costs)))\n",
    "# print('minibatch标准差 : '+ str(np.std(minibatch_costs)))\n",
    "# # print(minibatch_costs)\n",
    "# print(\"==============================================\")\n",
    "# # # training\n",
    "# # print(training_costs)\n",
    "# print('training均值 : '+ str(np.mean(training_costs)))\n",
    "# print('training标准方差 : '+ str(np.std(training_costs)))\n",
    "# print(\"==============================================\")\n",
    "# # # validation\n",
    "# # print(validation_costs)\n",
    "# print('validation均值 : '+ str(np.mean(validation_costs)))\n",
    "# print('validation标准方差 : '+ str(np.std(validation_costs)))\n",
    "# print(\"==============================================\")\n",
    "\n",
    "# plt.plot(minibatch_x_inds, minibatch_costs, 'k-', linewidth=1, label='cost minibatches')\n",
    "# plt.plot(iteration_x_inds, training_costs, 'r-', linewidth=2, label='cost full training set')\n",
    "# plt.plot(iteration_x_inds, validation_costs, 'b-', linewidth=3, label='cost validation set')\n",
    "\n",
    "# # Add labels to the plot\n",
    "# plt.xlabel('iteration')\n",
    "# plt.ylabel('$\\\\xi$', fontsize=15)\n",
    "# plt.title('Decrease of cost over not_backprop iteration')\n",
    "# plt.legend()\n",
    "# x1,x2,y1,y2 = plt.axis()\n",
    "# plt.axis((0,nb_of_iterations,0,6.0))\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # 问题: loss没有下降\n",
    "\n",
    "# # sig=0.01   #change the value of the noise\n",
    "# learning_rate = 0.001\n",
    "# # 定义weibull分布需要的参数\n",
    "# sigm = 1 # 可调整\n",
    "# k = 2 # 形状参数            \n",
    "# lambd = math.sqrt(2) * sigm # 比例参数\n",
    "# # 定义获取grad的需要的参数c\n",
    "# c = 1/(math.sqrt(2*math.pi*sigm*sigm)) \n",
    "\n",
    "# # 定义层\n",
    "# # Define a sample model to be trained on the data\n",
    "# hidden_neurons_1 = 20  # Number of neurons in the first hidden-layer\n",
    "# hidden_neurons_2 = 20  # Number of neurons in the second hidden-layer\n",
    "# # Create the model\n",
    "# layers = [] # Define a list of layers\n",
    "# # Add first hidden layer\n",
    "# layers.append(discreteLayer(X_train.shape[1], hidden_neurons_1,sigm,batch_size))   # nodesum = 20\n",
    "# layers.append(LogisticLayer())\n",
    "# # Add second hidden layer\n",
    "# layers.append(discreteLayer(21, hidden_neurons_2,sigm,batch_size))# nodesum = 20\n",
    "# layers.append(LogisticLayer())\n",
    "# # Add output layer\n",
    "# layers.append(discreteLayer(21, T_train.shape[1],sigm,batch_size)) #T_train.shape[1] =10 # nodesum = 10\n",
    "# layers.append(SoftmaxOutputLayer())\n",
    "\n",
    "# # 训练一个样本\n",
    "# x = XT_batches[0][0]\n",
    "# t = XT_batches[0][1]\n",
    "# # print(x.shape)\n",
    "# loss1 = 0\n",
    "# loss = 0\n",
    "# # 得到正负weibull的激活,选定结点的index\n",
    "# activations1,activations2,layer_index,node_index = forward_noise_step(x, layers,k,lambd,batch_size)\n",
    "# loss1 = layers[-1].get_cost(activations1[-1], t) # loss正\n",
    "# loss2 = layers[-1].get_cost(activations2[-1], t) # loss负\n",
    "# # 得到cost\n",
    "# loss = loss1 - loss2\n",
    "# # 获得梯度grad\n",
    "# grad = get_param_grad(activations1,layers,loss,c,layer_index)\n",
    "# print('grad均值 : '+ str(np.mean(grad)))\n",
    "# print('grad方差 : '+ \"{:.8f}\".format(np.var(grad)))\n",
    "# # # 更新选定结点的权重w\n",
    "# update_params2(layers,layer_index,grad,node_index,learning_rate)\n",
    "# # print(\"{:.8f}\".format(8.386465831669658e-06));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get results of test data\n",
    "# y_true = np.argmax(T_test, axis=1)  # Get the target outputs\n",
    "# activations = forward_step(X_test, layers)  # Get activation of test samples\n",
    "# y_pred = np.argmax(activations[-1], axis=1)  # Get the predictions made by the network\n",
    "# test_accuracy = metrics.accuracy_score(y_true, y_pred)  # Test set accuracy\n",
    "# print('The accuracy on the test set is {:.2f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show confusion table\n",
    "# conf_matrix = metrics.confusion_matrix(y_true, y_pred, labels=None)  # Get confustion matrix\n",
    "# # Plot the confusion table\n",
    "# class_names = ['${:d}$'.format(x) for x in range(0, 10)]  # Digit class names\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "# # Show class labels on each axis\n",
    "# ax.xaxis.tick_top()\n",
    "# major_ticks = range(0,10)\n",
    "# minor_ticks = [x + 0.5 for x in range(0, 10)]\n",
    "# ax.xaxis.set_ticks(major_ticks, minor=False)\n",
    "# ax.yaxis.set_ticks(major_ticks, minor=False)\n",
    "# ax.xaxis.set_ticks(minor_ticks, minor=True)\n",
    "# ax.yaxis.set_ticks(minor_ticks, minor=True)\n",
    "# ax.xaxis.set_ticklabels(class_names, minor=False, fontsize=15)\n",
    "# ax.yaxis.set_ticklabels(class_names, minor=False, fontsize=15)\n",
    "# # Set plot labels\n",
    "# ax.yaxis.set_label_position(\"right\")\n",
    "# ax.set_xlabel('Predicted label')\n",
    "# ax.set_ylabel('True label')\n",
    "# fig.suptitle('Confusion table', y=1.03, fontsize=15)\n",
    "# # Show a grid to seperate digits\n",
    "# ax.grid(b=True, which=u'minor')\n",
    "# # Color each grid cell according to the number classes predicted\n",
    "# ax.imshow(conf_matrix, interpolation='nearest', cmap='binary')\n",
    "# # Show the number of samples in each cell\n",
    "# for x in xrange(conf_matrix.shape[0]):\n",
    "#     for y in xrange(conf_matrix.shape[1]):\n",
    "#         color = 'w' if x == y else 'k'\n",
    "#         ax.text(x, y, conf_matrix[y,x], ha=\"center\", va=\"center\", color=color)       \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
