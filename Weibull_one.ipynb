{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduction : weibull noise,a sample in a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kzw\\AppData\\Local\\conda\\conda\\envs\\python27\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Python imports\n",
    "import numpy as np # Matrix and vector computation package\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib inline\n",
    "# Set the seed of the numpy random number generator so that the tutorial is reproducable\n",
    "np.random.seed(seed=1)\n",
    "from sklearn import datasets, cross_validation, metrics # data and evaluation utils\n",
    "from matplotlib.colors import colorConverter, ListedColormap # some plotting functions\n",
    "import itertools\n",
    "import collections\n",
    "from scipy import stats\n",
    "from scipy.stats import weibull_min\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_add_data(activ):\n",
    "    \"\"\"\n",
    "        参数activ : 每层的输出(激活)\n",
    "        作用 : 在数组头部插入数值1\n",
    "    \"\"\"\n",
    "    r = []\n",
    "    ret1 = np.append([1],activ)\n",
    "    r.append(ret1)\n",
    "    ret2 = np.array(r)\n",
    "    return ret2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0. ...  9.  0.  0.]\n",
      " [ 1.  0.  0. ... 16.  2.  0.]\n",
      " [ 1.  0.  2. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 1.  0.  0. ...  1.  0.  0.]\n",
      " [ 1.  0.  0. ... 11.  2.  0.]\n",
      " [ 1.  0.  0. ... 12.  0.  0.]]\n",
      "(1078L, 65L)\n",
      "(65L,)\n"
     ]
    }
   ],
   "source": [
    "# 原来的code\n",
    "# digits = datasets.load_digits()\n",
    "\n",
    "# T = np.zeros((digits.target.shape[0],10))\n",
    "# T[np.arange(len(T)), digits.target] += 1\n",
    "\n",
    "# X_train, X_test, T_train, T_test = cross_validation.train_test_split(\n",
    "#     digits.data, T, test_size=0.4)\n",
    "# X_validation, X_test, T_validation, T_test = cross_validation.train_test_split(\n",
    "#     X_test, T_test, test_size=0.5)\n",
    "\n",
    "# 更改后的code\n",
    "# 在所有样本的头部添加数值1\n",
    "digits = datasets.load_digits()\n",
    "data = digits.data # array\n",
    "target_list = [] # list\n",
    "target_data = []\n",
    "# single_array = np.array([1])\n",
    "for i in range(len(data)):\n",
    "    # 取出数据\n",
    "    a = data[i]\n",
    "    # 头部添加1\n",
    "    a_temp = array_add_data(a)[0]\n",
    "    # 添加到总的array中\n",
    "    target_list.append(a_temp)\n",
    "target_data = np.array(target_list) \n",
    "\n",
    "T = np.zeros((digits.target.shape[0],10))\n",
    "T[np.arange(len(T)), digits.target] += 1\n",
    "\n",
    "X_train, X_test, T_train, T_test = cross_validation.train_test_split(\n",
    "    target_data, T, test_size=0.4)\n",
    "X_validation, X_test, T_validation, T_test = cross_validation.train_test_split(\n",
    "    X_test, T_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x100 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 1), dpi=100)\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(1,10,i+1)\n",
    "    ax.matshow(digits.images[i], cmap='binary') \n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the non-linear functions used\n",
    "def logistic(z): \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_deriv(y):  # Derivative of logistic function\n",
    "    return np.multiply(y, (1 - y))\n",
    "    \n",
    "def softmax(z): \n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layers used in this model\n",
    "class Layer(object):\n",
    "    \"\"\"Base class for the different layers.\n",
    "    Defines base methods and documentation of methods.\"\"\"\n",
    "    \n",
    "    def get_params_iter(self):\n",
    "        \"\"\"Return an iterator over the parameters (if any).\n",
    "        The iterator has the same order as get_params_grad.\n",
    "        The elements returned by the iterator are editable in-place.\"\"\"\n",
    "        return []\n",
    "    \n",
    "    def get_params_grad(self, X, output_grad):\n",
    "        \"\"\"Return a list of gradients over the parameters.\n",
    "        The list has the same order as the get_params_iter iterator.\n",
    "        X is the input.\n",
    "        output_grad is the gradient at the output of this layer.\n",
    "        \"\"\"\n",
    "        return []\n",
    "      \n",
    "    def get_params_update(self, X, loss):\n",
    "        \"\"\"Return a list of gradients over the parameters.\n",
    "        The list has the same order as the get_params_iter iterator.\n",
    "        X is the input.\n",
    "        output_grad is the gradient at the output of this layer.\n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def add_noise(self):\n",
    "        pass\n",
    "    \n",
    "    def remove_noise(self):\n",
    "        pass\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step linear transformation.\n",
    "        X is the input.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad=None, T=None):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\n",
    "        Y is the pre-computed output of this layer (not needed in this case).\n",
    "        output_grad is the gradient at the output of this layer \n",
    "         (gradient at input of next layer).\n",
    "        Output layer uses targets T to compute the gradient based on the \n",
    "         output error instead of output_grad\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class discreteLayer(Layer):\n",
    "    \"\"\"The linear layer performs a linear transformation to its input.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_in, n_out,sig):\n",
    "        \"\"\"Initialize hidden layer parameters.\n",
    "        n_in is the number of input variables.\n",
    "        n_out is the number of output variables.\"\"\"\n",
    "        self.W = np.random.randn(n_in, n_out) * 0.1\n",
    "        self.b = np.zeros(n_out)\n",
    "        self.sig=sig\n",
    "        self.noise=0\n",
    "    # training set 训练时: 添加高斯noise\n",
    "    def add_noise(self):\n",
    "        self.noise=np.random.randn(self.b.shape[0])*self.sig\n",
    "    \n",
    "    # 移除选定结点的高斯noise\n",
    "    def remove_targetnode_gaosi_noise(self,node_index):\n",
    "        self.noise[node_index - 1] = 0 \n",
    "    \n",
    "    # validation set 验证时: 移除高斯noise\n",
    "    def remove_noise(self):\n",
    "        self.noise = np.zeros(self.b.shape[0], dtype=float, order='C')\n",
    "    \n",
    "    def get_params_iter(self):\n",
    "        \"\"\"Return an iterator over the parameters.\"\"\"\n",
    "        return self.W\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step linear transformation.\"\"\"\n",
    "        xw = X.dot(self.W) + self.noise\n",
    "        return xw\n",
    "        \n",
    "#     def get_params_grad(self, X, output_grad):\n",
    "#         \"\"\"Return a list of gradients over the parameters.\"\"\"\n",
    "#         JW = X.T.dot(output_grad)\n",
    "#         Jb = np.sum(output_grad, axis=0)\n",
    "#         return [g for g in itertools.chain(np.nditer(JW), np.nditer(Jb))]\n",
    "    \n",
    "#     def get_params_update(self, ac, loss):\n",
    "#         \"\"\"Return a list of gradients over the parameters.\"\"\"\n",
    "#         Z = np.expand_dims(self.noise,axis=1)        \n",
    "#         gradW = (ac.T.dot(Z.T)*loss)/(self.sigma*self.sigma)\n",
    "#         gradb=self.noise*loss\n",
    "#         return [g for g in itertools.chain(np.nditer(gradW),np.nditer(gradb))]\n",
    "        \n",
    "#     def get_input_grad(self, Y, output_grad):\n",
    "#         \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "#         return output_grad.dot(self.W.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticLayer(Layer):\n",
    "    \"\"\"The logistic layer applies the logistic function to its inputs.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.noise = 0\n",
    "        self.b = 0\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        # 在头部插入数值1\n",
    "        Y = array_add_data(logistic(X))\n",
    "        return Y\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        return np.multiply(logistic_deriv(Y), output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxOutputLayer(Layer):\n",
    "    \"\"\" \n",
    "        输出层逻辑函数\n",
    "    \"\"\"  \n",
    "    def __init__(self):\n",
    "        self.noise = 0\n",
    "        self.b = 0\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return softmax(X)\n",
    "    \n",
    "    def get_input_grad(self, Y, T):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\n",
    "            Y:最后层激活\n",
    "            T:target\n",
    "        \"\"\"\n",
    "        return (Y - T) / Y.shape[0]   # /Y.shape[0] -->取平均值(输入的所有样本)\n",
    "    \n",
    "    def get_cost(self, A, T):\n",
    "        \"\"\"Return the cost at the output of this output layer.\"\"\"\n",
    "        re = - np.multiply(T, np.log(A)).sum() / len(A)\n",
    "        return re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.018889834413934797\n"
     ]
    }
   ],
   "source": [
    "def get_weibull(k,lambd):\n",
    "    \"\"\"\n",
    "        从weibull分布中随机取一个数\n",
    "    \"\"\"\n",
    "#     weibull_array = stats.exponweib.rvs(a=k, c=k, scale=lambd, loc=0, size=100)   # 两个形状参数什么意思 ?????????? \n",
    "    weibull_array = weibull_min.rvs(k, loc=0, scale=lambd, size=100)\n",
    "    d = random.choice(weibull_array)\n",
    "    return d\n",
    "\n",
    "sigm = 0.01             # adjust\n",
    "k = 2 # 形状参数            \n",
    "lambd = (math.sqrt(2)) * sigm # 比例参数\n",
    "print(get_weibull(k,lambd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_index():\n",
    "    \"\"\"\n",
    "        得到随机结点的index\n",
    "    \"\"\"\n",
    "    # 得到第几层\n",
    "    layer_index = random.choice([1,2,3])\n",
    "    # 得到第几个结点\n",
    "    if (layer_index == 1) or (layer_index == 2):\n",
    "        node_index = random.choice([1,2,3,4,5,6,7,8,9,10,12,13,14,15,16,17,18,19,20])\n",
    "    else:\n",
    "        node_index = random.choice([1,2,3,4,5,6,7,8,9,10])\n",
    "    #将结点与逻辑层对应\n",
    "    if layer_index == 2:\n",
    "        layer_index = 3\n",
    "    elif layer_index == 3:\n",
    "        layer_index = 5\n",
    "    return layer_index,node_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_noise_step0(input_samples, layers):\n",
    "    \"\"\"\n",
    "        validation set 验证时的前向传播\n",
    "    \"\"\"\n",
    "    activations = [input_samples] \n",
    "    X = input_samples\n",
    "    for layer in layers:\n",
    "        layer.remove_noise()\n",
    "        Y = layer.get_output(X) \n",
    "        activations.append(Y) \n",
    "        X = activations[-1]  \n",
    "    return activations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set 训练时的前向传播\n",
    "def forward_noise_step(input_samples, layers,k,lambd):\n",
    "    \"\"\"\n",
    "        return:\n",
    "                正向weibull和反向weibull对应的activation\n",
    "                选定结点的layer_index,node_index\n",
    "    \"\"\"\n",
    "   \n",
    "    # 得到layer,node的index\n",
    "    layer_index,node_index = get_node_index()\n",
    "    # print('层:'+str(layer_index)+'  结点:'+str(node_index)) #层:3  结点:7\n",
    "    # 抽取一个weibull数据\n",
    "    weibull_noise1 = get_weibull(k,lambd) #正向weibull\n",
    "    weibull_noise2 = weibull_noise1 * (-1)#反向\n",
    "\n",
    "# 移除选定结点的高斯noise\n",
    "    # get正向weibull的activation1\n",
    "    activations1 = [input_samples] # List of layer activations\n",
    "    X = input_samples\n",
    "    for index in range(len(layers)):\n",
    "        index_target1 = index + 1 # 指定当前在第几层\n",
    "        layer = layers[index]\n",
    "        # 给选定的结点加weibull noise && 移除高斯noise\n",
    "        if index_target1 == layer_index: \n",
    "            layer.add_noise()\n",
    "            # 移除指定结点的高斯noise\n",
    "            layer.remove_targetnode_gaosi_noise(node_index)\n",
    "            Y = layer.get_output(X)  # z=xw+b都是一维数组     type(Y):array\n",
    "            # 若是多个样本就需要循环change\n",
    "            Y[0][node_index - 1] =  Y[0][node_index - 1] + weibull_noise1  # 此处的if只针对线性层:所以index=node_index - 1\n",
    "        else:\n",
    "            layer.add_noise()\n",
    "            Y = layer.get_output(X)\n",
    "        activations1.append(Y)  # 存储输出以供将来处理\n",
    "        X = activations1[-1]  # 将前一层的输出设置为当前层的激活\n",
    "    \n",
    "    # get反向weibull的activation2\n",
    "    activations2 = [input_samples] \n",
    "    X = input_samples\n",
    "    for index in range(len(layers)):\n",
    "        index_target2 = index + 1\n",
    "        layer = layers[index]\n",
    "        # 给选定的结点加weibull noise && 移除高斯noise\n",
    "        if index_target2 == layer_index: \n",
    "            # 移除指定结点的高斯noise\n",
    "            layer.remove_targetnode_gaosi_noise(node_index)\n",
    "            Y = layer.get_output(X)  # z=xw+b都是一维数组 \n",
    "            Y[0][node_index - 1] =  Y[0][node_index - 1] + weibull_noise2  # 1.0940626938073394\n",
    "        else:\n",
    "            Y = layer.get_output(X)\n",
    "        activations2.append(Y)  \n",
    "        X = activations2[-1]  # 将前一层的输出设置为当前层的激活\n",
    "    return activations1,activations2,layer_index,node_index  # 返回正向激活,反向激活"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_grad(ac,layers,loss,c,layer_index):\n",
    "    \"\"\"\n",
    "        获得选定结点w的grad\n",
    "    \"\"\"\n",
    "    # 选定要update的层\n",
    "    layer = layers[layer_index - 1]\n",
    "    # 获得层对应的输入\n",
    "    x = ac[layer_index - 1]\n",
    "    # 计算grad\n",
    "    grad = (-1)*c*x*(loss)\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params2(layers,layer_index,grad,node_index,learning_rate):\n",
    "    \"\"\"\n",
    "        更新选定节点的w权重\n",
    "    \"\"\"\n",
    "    # 选定要update的层\n",
    "    layer = layers[layer_index - 1]  \n",
    "    # 获得对应层的权重w\n",
    "    w = layer.get_params_iter()\n",
    "    # 更新\n",
    "    w[:,node_index - 1] -= learning_rate * grad[0]\n",
    "    layer.W = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建minibatches\n",
    "batch_size = 1# 一批1各样本\n",
    "nb_of_batches = X_train.shape[0] / batch_size  \n",
    "# Create batches (X,Y) from the training set\n",
    "XT_batches = zip(\n",
    "    np.array_split(X_train, nb_of_batches, axis=0),  \n",
    "    np.array_split(T_train, nb_of_batches, axis=0))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "# 定义超参数\n",
    "sig = 0.01              # adjust\n",
    "learning_rate = 0.001   # adjust\n",
    "# weibull分布需要的参数\n",
    "sigm = 0.0001             # adjust\n",
    "k = 2 # 形状参数            \n",
    "lambd = (math.sqrt(2)) * sigm # 比例参数\n",
    "# get_grad需要的参数c\n",
    "c = 1/(math.sqrt(2*math.pi*sigm*sigm))\n",
    "\n",
    "# Define a sample model to be trained on the data\n",
    "hidden_neurons_1 = 20  # Number of neurons in the first hidden-layer\n",
    "hidden_neurons_2 = 20  # Number of neurons in the second hidden-layer\n",
    "# Create the model\n",
    "layers = [] # Define a list of layers\n",
    "# Add first hidden layer\n",
    "layers.append(discreteLayer(X_train.shape[1], hidden_neurons_1,sig))   # nodesum = 20\n",
    "layers.append(LogisticLayer())\n",
    "# Add second hidden layer\n",
    "layers.append(discreteLayer(21, hidden_neurons_2,sig))# shape(20,20) -> (21,20): 因为上一层的激活增加一个数\n",
    "layers.append(LogisticLayer())\n",
    "# Add output layer\n",
    "layers.append(discreteLayer(21, T_train.shape[1],sig)) # shape(20,10) -> (21,100): 因为上一层的激活增加一个数\n",
    "layers.append(SoftmaxOutputLayer())\n",
    "\n",
    "minibatch_costs = []\n",
    "training_costs = []\n",
    "validation_costs = []\n",
    "# 训练次数\n",
    "max_nb_of_iterations = 60 \n",
    "# 开始训练\n",
    "for iteration in range(max_nb_of_iterations): \n",
    "    loss1 = 0\n",
    "    loss2 = 0\n",
    "    for x, t in XT_batches:  \n",
    "        # 得到正负weibull的激活,选定结点的index\n",
    "        activations1,activations2,layer_index,node_index = forward_noise_step(x, layers,k,lambd)\n",
    "        # 得到正负loss\n",
    "        loss1 = layers[-1].get_cost(activations1[-1], t) # loss正\n",
    "        loss2 = layers[-1].get_cost(activations2[-1], t) # loss负\n",
    "        # 得到cost\n",
    "        loss = loss1 - loss2\n",
    "        # 获得梯度grad\n",
    "        grad = get_param_grad(activations1,layers,loss,c,layer_index)\n",
    "        # 更新选定结点的权重w\n",
    "        update_params2(layers,layer_index,grad,node_index,learning_rate)\n",
    "        \n",
    "    # Get minibatch cost\n",
    "    minibatch_costs.append(loss1)\n",
    "#     print(\"loss1 : \" + str(loss1))\n",
    "#     # Get full training cost\n",
    "#     activations = forward_noise_step0(X_train, layers)\n",
    "#     train_cost = layers[-1].get_cost(activations[-1], T_train)\n",
    "#     training_costs.append(train_cost)\n",
    "#     # Get full validation cost\n",
    "#     activations = forward_noise_step0(X_validation, layers)\n",
    "#     validation_cost = layers[-1].get_cost(activations[-1], T_validation)\n",
    "#     validation_costs.append(validation_cost)\n",
    "    #if len(validation_costs) > 3:\n",
    "        # Stop training if the cost on the validation set doesn't decrease\n",
    "        #  for 3 iterations\n",
    "        #if validation_costs[-1] >= validation_costs[-2] >= validation_costs[-3]:\n",
    "         #   break\n",
    "    # 显示迭代次数\n",
    "    if (iteration + 1) % 10 == 0:\n",
    "        print(iteration + 1) \n",
    "nb_of_iterations = iteration + 1  # The number of iterations that have been executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minibatch均值 : 2.1642501012218687\n",
      "minibatch方差 : 0.03229074753290834\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the minibatch, full training set, and validation costs\n",
    "minibatch_x_inds = np.linspace(1, max_nb_of_iterations, num=max_nb_of_iterations, endpoint=True)\n",
    "iteration_x_inds = np.linspace(1, max_nb_of_iterations, num=max_nb_of_iterations, endpoint=True)\n",
    "plt.plot(minibatch_x_inds, minibatch_costs, 'k-', linewidth=0.5, label='cost minibatches')\n",
    "# plt.plot(iteration_x_inds, training_costs, 'r-', linewidth=2, label='cost full training set')\n",
    "# plt.plot(iteration_x_inds, validation_costs, 'b-', linewidth=3, label='cost validation set')\n",
    "\n",
    "# Add labels to the plot\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('$\\\\xi$', fontsize=15)\n",
    "plt.title('Decrease of cost over backprop iteration')\n",
    "plt.legend()\n",
    "x1,x2,y1,y2 = plt.axis()\n",
    "plt.axis((0,nb_of_iterations,0,6.0))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results of test data\n",
    "y_true = np.argmax(T_test, axis=1)  # Get the target outputs\n",
    "activations = forward_step(X_test, layers)  # Get activation of test samples\n",
    "y_pred = np.argmax(activations[-1], axis=1)  # Get the predictions made by the network\n",
    "test_accuracy = metrics.accuracy_score(y_true, y_pred)  # Test set accuracy\n",
    "print('The accuracy on the test set is {:.2f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show confusion table\n",
    "# conf_matrix = metrics.confusion_matrix(y_true, y_pred, labels=None)  # Get confustion matrix\n",
    "# # Plot the confusion table\n",
    "# class_names = ['${:d}$'.format(x) for x in range(0, 10)]  # Digit class names\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111)\n",
    "# # Show class labels on each axis\n",
    "# ax.xaxis.tick_top()\n",
    "# major_ticks = range(0,10)\n",
    "# minor_ticks = [x + 0.5 for x in range(0, 10)]\n",
    "# ax.xaxis.set_ticks(major_ticks, minor=False)\n",
    "# ax.yaxis.set_ticks(major_ticks, minor=False)\n",
    "# ax.xaxis.set_ticks(minor_ticks, minor=True)\n",
    "# ax.yaxis.set_ticks(minor_ticks, minor=True)\n",
    "# ax.xaxis.set_ticklabels(class_names, minor=False, fontsize=15)\n",
    "# ax.yaxis.set_ticklabels(class_names, minor=False, fontsize=15)\n",
    "# # Set plot labels\n",
    "# ax.yaxis.set_label_position(\"right\")\n",
    "# ax.set_xlabel('Predicted label')\n",
    "# ax.set_ylabel('True label')\n",
    "# fig.suptitle('Confusion table', y=1.03, fontsize=15)\n",
    "# # Show a grid to seperate digits\n",
    "# ax.grid(b=True, which=u'minor')\n",
    "# # Color each grid cell according to the number classes predicted\n",
    "# ax.imshow(conf_matrix, interpolation='nearest', cmap='binary')\n",
    "# # Show the number of samples in each cell\n",
    "# for x in xrange(conf_matrix.shape[0]):\n",
    "#     for y in xrange(conf_matrix.shape[1]):\n",
    "#         color = 'w' if x == y else 'k'\n",
    "#         ax.text(x, y, conf_matrix[y,x], ha=\"center\", va=\"center\", color=color)       \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
