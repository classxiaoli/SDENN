{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 描述 : 只加高斯noise batchsize=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import numpy as np # Matrix and vector computation package\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib inline\n",
    "# Set the seed of the numpy random number generator so that the tutorial is reproducable\n",
    "np.random.seed(seed=1)\n",
    "from sklearn import datasets, cross_validation, metrics # data and evaluation utils\n",
    "from matplotlib.colors import colorConverter, ListedColormap # some plotting functions\n",
    "import itertools\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "\n",
    "T = np.zeros((digits.target.shape[0],10))\n",
    "T[np.arange(len(T)), digits.target] += 1\n",
    "\n",
    "X_train, X_test, T_train, T_test = cross_validation.train_test_split(\n",
    "    digits.data, T, test_size=0.4)\n",
    "X_validation, X_test, T_validation, T_test = cross_validation.train_test_split(\n",
    "    X_test, T_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x100 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 1), dpi=100)\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(1,10,i+1)\n",
    "    ax.matshow(digits.images[i], cmap='binary') \n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the non-linear functions used\n",
    "def logistic(z): \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_deriv(y):  # Derivative of logistic function\n",
    "    return np.multiply(y, (1 - y))\n",
    "    \n",
    "def softmax(z): \n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layers used in this model\n",
    "class Layer(object):\n",
    "    \"\"\"Base class for the different layers.\n",
    "    Defines base methods and documentation of methods.\"\"\"\n",
    "    \n",
    "    def get_params_iter(self):\n",
    "        \"\"\"Return an iterator over the parameters (if any).\n",
    "        The iterator has the same order as get_params_grad.\n",
    "        The elements returned by the iterator are editable in-place.\"\"\"\n",
    "        return []\n",
    "    \n",
    "    def get_params_grad(self, X, output_grad):\n",
    "        \"\"\"Return a list of gradients over the parameters.\n",
    "        The list has the same order as the get_params_iter iterator.\n",
    "        X is the input.\n",
    "        output_grad is the gradient at the output of this layer.\n",
    "        \"\"\"\n",
    "        return []\n",
    "      \n",
    "    def get_params_update(self, X, loss,for_nodesum,cur_nodesum):\n",
    "        \"\"\"Return a list of gradients over the parameters.\n",
    "        The list has the same order as the get_params_iter iterator.\n",
    "        X is the input.\n",
    "        output_grad is the gradient at the output of this layer.\n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def add_noise(self):\n",
    "        pass\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step linear transformation.\n",
    "        X is the input.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad=None, T=None):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\n",
    "        Y is the pre-computed output of this layer (not needed in this case).\n",
    "        output_grad is the gradient at the output of this layer \n",
    "         (gradient at input of next layer).\n",
    "        Output layer uses targets T to compute the gradient based on the \n",
    "         output error instead of output_grad\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class discreteLayer(Layer):\n",
    "    \"\"\"The linear layer performs a linear transformation to its input.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_in, n_out,sigma,sig):\n",
    "        \"\"\"Initialize hidden layer parameters.\n",
    "        n_in is the number of input variables.\n",
    "        n_out is the number of output variables.\"\"\"\n",
    "        self.W = np.random.randn(n_in, n_out) * 0.1\n",
    "        self.b = np.zeros(n_out)\n",
    "        #记录当前层的节点数\n",
    "        self.nodesum = n_out\n",
    "        self.sigma=sigma\n",
    "        self.sig=sig\n",
    "        self.noise=0\n",
    "    \n",
    "    def add_noise(self):\n",
    "        self.noise=np.random.randn(self.b.shape[0])*self.sig\n",
    "#         self.noise=np.random.weibull(5,self.b.shape[0])\n",
    "        \n",
    "    def get_params_iter(self):\n",
    "        \"\"\"Return an iterator over the parameters.\"\"\"\n",
    "        return itertools.chain(np.nditer(self.W, op_flags=['readwrite']),\n",
    "                               np.nditer(self.b, op_flags=['readwrite']))\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step linear transformation.\"\"\"\n",
    "        return X.dot(self.W) + self.noise + self.b\n",
    "        \n",
    "    def get_params_grad(self, X, output_grad):\n",
    "        \"\"\"Return a list of gradients over the parameters.\"\"\"\n",
    "        JW = X.T.dot(output_grad)\n",
    "        Jb = np.sum(output_grad, axis=0)\n",
    "        return [g for g in itertools.chain(np.nditer(JW), np.nditer(Jb))]\n",
    "    \n",
    "    def get_params_update(self, ac, loss,for_nodesum,cur_nodesum):\n",
    "        \"\"\"Return a list of gradients over the parameters.\"\"\"\n",
    "        Z=np.expand_dims(self.noise,axis=1)           #作用: z.shape (10L,) -->   (10L, 1L)\n",
    "        #计算一个batch(32个样本)的平均梯度\n",
    "        gradW = np.zeros((for_nodesum,cur_nodesum))\n",
    "        for a in ac:\n",
    "            a = np.expand_dims(a,axis=1)\n",
    "            grad = (a.dot(Z.T)*loss)/(self.sigma*self.sigma)\n",
    "            gradW = grad + gradW\n",
    "        gradW = gradW/ac.shape[0]\n",
    "        gradb=self.noise*loss\n",
    "        return [g for g in itertools.chain(np.nditer(gradW),np.nditer(gradb))]\n",
    "        \n",
    "    def get_input_grad(self, Y, output_grad):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        return output_grad.dot(self.W.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import izip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticLayer(Layer):\n",
    "    \"\"\"The logistic layer applies the logistic function to its inputs.\"\"\"\n",
    "    \n",
    "    def __init__(self,nodesum):\n",
    "        #记录当前层(三层中的某一层)的节点数\n",
    "        self.nodesum = nodesum\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return logistic(X)\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        return np.multiply(logistic_deriv(Y), output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxOutputLayer(Layer):\n",
    "    \"\"\"The softmax output layer computes the classification propabilities at the output.\"\"\"\n",
    "    \n",
    "    def __init__(self,nodesum):\n",
    "        #记录当前层(三层中的某一层)的节点数\n",
    "        self.nodesum = nodesum\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "#         print(\"softmax的输入:\")\n",
    "#         print(X.shape)\n",
    "        return softmax(X)\n",
    "    \n",
    "    def get_input_grad(self, Y, T):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\n",
    "            Y:最后层激活\n",
    "            T:target\n",
    "        \"\"\"\n",
    "        return (Y - T) / Y.shape[0]   # /Y.shape[0] -->取平均值(输入的所有样本)\n",
    "    \n",
    "    def get_cost(self, Y, T):\n",
    "        \"\"\"Return the cost at the output of this output layer.\"\"\"\n",
    "        return - np.multiply(T, np.log(Y)).sum() / Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the forward propagation step as a method.\n",
    "def forward_noise_step(input_samples, layers):\n",
    "    activations = [input_samples] # List of layer activations\n",
    "    X = input_samples\n",
    "    for layer in layers:\n",
    "        layer.add_noise()\n",
    "        Y = layer.get_output(X) \n",
    "        activations.append(Y) \n",
    "#         print(Y.shape)\n",
    "        X = activations[-1]  \n",
    "    return activations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the backward propagation step as a method\n",
    "def not_backward_step(activations, targets, layers, loss):\n",
    "    \"\"\"\n",
    "    Perform the backpropagation step over all the layers and return the parameter gradients.\n",
    "    Input:\n",
    "        activations: A list of forward step activations where the activation at \n",
    "            each index i+1 corresponds to the activation of layer i in layers. \n",
    "            activations[0] contains the input samples. \n",
    "        targets: The output targets of the output layer.\n",
    "        layers: A list of Layers corresponding that generated the outputs in activations.\n",
    "    Output:\n",
    "        A list of parameter gradients where the gradients at each index corresponds to\n",
    "        the parameters gradients of the layer at the same index in layers. \n",
    "    \"\"\"\n",
    "    param_grads = collections.deque()  # List of parameter gradients for each layer\n",
    "    output_grad = None  # The error gradient at the output of the current layer\n",
    "    # Propagate the error backwards through all the layers.\n",
    "    #  Use reversed to iterate backwards over the list of layers.\n",
    "    i = 1\n",
    "    l = len(layers)\n",
    "    for index in reversed(range(len(layers))):   \n",
    "        layer = layers[index]\n",
    "        for_nodesum = 0\n",
    "        #得到当前层和前一层的节点数\n",
    "        cur_nodesum = layer.nodesum\n",
    "        if (layer is layers[0]):\n",
    "            for_nodesum = 64\n",
    "        else:\n",
    "            for_nodesum = layers[index -1].nodesum\n",
    "            \n",
    "        Y = activations.pop() \n",
    "        ac = activations[-1]\n",
    "        grads=layer.get_params_update(ac,loss,for_nodesum,cur_nodesum)\n",
    "        param_grads.appendleft(grads)\n",
    "    return list(param_grads) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the minibatches\n",
    "\n",
    "X_train = X_train[0:1024,:]\n",
    "T_train = T_train[0:1024,:]\n",
    "\n",
    "batch_size = 32# 一批32各样本\n",
    "nb_of_batches = X_train.shape[0] / batch_size  # 32批\n",
    "# Create batches (X,Y) from the training set\n",
    "XT_batches = zip(\n",
    "    np.array_split(X_train, nb_of_batches, axis=0),  \n",
    "    np.array_split(T_train, nb_of_batches, axis=0))  \n",
    "# print(nb_of_batches)\n",
    "# print(X_train.shape)\n",
    "# print(T_train.shape)\n",
    "# print(len(XT_batches))\n",
    "# print(XT_batches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a method to update the parameters\n",
    "def update_params(layers, param_grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Function to update the parameters of the given layers with the given gradients\n",
    "    by gradient descent with the given learning rate.\n",
    "    \"\"\"\n",
    "    for layer, layer_backprop_grads in zip(layers, param_grads):\n",
    "        for param, grad in itertools.izip(layer.get_params_iter(), layer_backprop_grads):\n",
    "            # The parameter returned by the iterator point to the memory space of\n",
    "            #  the original layer and can thus be modified inplace.\n",
    "            param -= learning_rate * grad  # Update each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# Perform backpropagation for noise \n",
    "# initalize some lists to store the cost for future analysis        \n",
    "sigma=0.93 #get_params_update\n",
    "sig=0.01   #change the value of the noise\n",
    "learning_rate = 0.05\n",
    "# Define a sample model to be trained on the data\n",
    "hidden_neurons_1 = 20  # Number of neurons in the first hidden-layer\n",
    "hidden_neurons_2 = 20  # Number of neurons in the second hidden-layer\n",
    "# Create the model\n",
    "layers = [] # Define a list of layers\n",
    "# Add first hidden layer\n",
    "#layers.append(LinearLayer(X_train.shape[1], hidden_neurons_1))\n",
    "layers.append(discreteLayer(X_train.shape[1], hidden_neurons_1,sigma,sig))   # nodesum = 20\n",
    "layers.append(LogisticLayer(20))\n",
    "# Add second hidden layer\n",
    "#layers.append(LinearLayer(hidden_neurons_1, hidden_neurons_2))\n",
    "layers.append(discreteLayer(hidden_neurons_1, hidden_neurons_2,sigma,sig))# nodesum = 20\n",
    "layers.append(LogisticLayer(20))\n",
    "# Add output layer\n",
    "#layers.append(LinearLayer(hidden_neurons_2, T_train.shape[1]))\n",
    "layers.append(discreteLayer(hidden_neurons_2, T_train.shape[1],sigma,sig)) #T_train.shape[1] =10 # nodesum = 10\n",
    "layers.append(SoftmaxOutputLayer(10))\n",
    "\n",
    "minibatch_costs = []\n",
    "training_costs = []\n",
    "validation_costs = []\n",
    "\n",
    "max_nb_of_iterations = 20 # Train for a maximum of 20 iterations\n",
    "# Train for the maximum number of iterations\n",
    "for iteration in range(max_nb_of_iterations): #80次\n",
    "    for x, t in XT_batches:  #  32次 \n",
    "        acti = forward_noise_step(x, layers)\n",
    "        los = layers[-1].get_cost(acti[-1], t)\n",
    "        gra = not_backward_step(acti, t, layers, los)\n",
    "        update_params(layers, gra, learning_rate)\n",
    "        \n",
    "    minibatch_costs.append(los)\n",
    "    activations = forward_noise_step(X_train, layers)\n",
    "    train_cost = layers[-1].get_cost(activations[-1], T_train)\n",
    "    training_costs.append(train_cost)\n",
    "    # Get full validation cost\n",
    "    activations = forward_noise_step(X_validation, layers)\n",
    "    validation_cost = layers[-1].get_cost(activations[-1], T_validation)\n",
    "    validation_costs.append(validation_cost)\n",
    "    #if len(validation_costs) > 3:\n",
    "        # Stop training if the cost on the validation set doesn't decrease\n",
    "        #  for 3 iterations\n",
    "        #if validation_costs[-1] >= validation_costs[-2] >= validation_costs[-3]:\n",
    "         #   break\n",
    "    if (iteration + 1) % 10 == 0:\n",
    "        print(iteration + 1) \n",
    "nb_of_iterations = iteration + 1  # The number of iterations that have been executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个batch的前向,后向传播\n",
    "x = XT_batches[0][0]\n",
    "t = XT_batches[0][1]\n",
    "# print(x.shape)\n",
    "# print(t.shape)\n",
    "acti = forward_noise_step(x, layers)\n",
    "# print(len(acti))\n",
    "los = layers[-1].get_cost(acti[-1], t)\n",
    "# print(los)\n",
    "gra = not_backward_step(acti, t, layers, los)\n",
    "# for gr in gra:\n",
    "#     print('梯度')\n",
    "#     print(gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the minibatch, full training set, and validation costs\n",
    "minibatch_x_inds = np.linspace(1, max_nb_of_iterations, num=max_nb_of_iterations, endpoint=True)\n",
    "iteration_x_inds = np.linspace(1, max_nb_of_iterations, num=max_nb_of_iterations, endpoint=True)\n",
    "# Plot the cost over the iterations\n",
    "\n",
    "# print(minibatch_x_inds)\n",
    "# print(len(minibatch_x_inds))\n",
    "# print(len(minibatch_costs))\n",
    "\n",
    "# print(minibatch_costs)\n",
    "# print('\\n')\n",
    "# print(training_costs)\n",
    "# print('\\n')\n",
    "# print(validation_costs)\n",
    "\n",
    "plt.plot(minibatch_x_inds, minibatch_costs, 'k-', linewidth=0.5, label='cost minibatches')\n",
    "plt.plot(iteration_x_inds, training_costs, 'r-', linewidth=2, label='cost full training set')\n",
    "plt.plot(iteration_x_inds, validation_costs, 'b-', linewidth=3, label='cost validation set')\n",
    "# Add labels to the plot\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('$\\\\xi$', fontsize=15)\n",
    "plt.title('Decrease of cost over backprop iteration')\n",
    "plt.legend()\n",
    "x1,x2,y1,y2 = plt.axis()\n",
    "plt.axis((0,nb_of_iterations,0,6.0))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'forward_step' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-945-3d5cae6a72e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Get results of test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Get the target outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mactivations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Get activation of test samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Get the predictions made by the network\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Test set accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'forward_step' is not defined"
     ]
    }
   ],
   "source": [
    "# Get results of test data\n",
    "y_true = np.argmax(T_test, axis=1)  # Get the target outputs\n",
    "activations = forward_step(X_test, layers)  # Get activation of test samples\n",
    "y_pred = np.argmax(activations[-1], axis=1)  # Get the predictions made by the network\n",
    "test_accuracy = metrics.accuracy_score(y_true, y_pred)  # Test set accuracy\n",
    "print('The accuracy on the test set is {:.2f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confusion table\n",
    "conf_matrix = metrics.confusion_matrix(y_true, y_pred, labels=None)  # Get confustion matrix\n",
    "# Plot the confusion table\n",
    "class_names = ['${:d}$'.format(x) for x in range(0, 10)]  # Digit class names\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "# Show class labels on each axis\n",
    "ax.xaxis.tick_top()\n",
    "major_ticks = range(0,10)\n",
    "minor_ticks = [x + 0.5 for x in range(0, 10)]\n",
    "ax.xaxis.set_ticks(major_ticks, minor=False)\n",
    "ax.yaxis.set_ticks(major_ticks, minor=False)\n",
    "ax.xaxis.set_ticks(minor_ticks, minor=True)\n",
    "ax.yaxis.set_ticks(minor_ticks, minor=True)\n",
    "ax.xaxis.set_ticklabels(class_names, minor=False, fontsize=15)\n",
    "ax.yaxis.set_ticklabels(class_names, minor=False, fontsize=15)\n",
    "# Set plot labels\n",
    "ax.yaxis.set_label_position(\"right\")\n",
    "ax.set_xlabel('Predicted label')\n",
    "ax.set_ylabel('True label')\n",
    "fig.suptitle('Confusion table', y=1.03, fontsize=15)\n",
    "# Show a grid to seperate digits\n",
    "ax.grid(b=True, which=u'minor')\n",
    "# Color each grid cell according to the number classes predicted\n",
    "ax.imshow(conf_matrix, interpolation='nearest', cmap='binary')\n",
    "# Show the number of samples in each cell\n",
    "for x in xrange(conf_matrix.shape[0]):\n",
    "    for y in xrange(conf_matrix.shape[1]):\n",
    "        color = 'w' if x == y else 'k'\n",
    "        ax.text(x, y, conf_matrix[y,x], ha=\"center\", va=\"center\", color=color)       \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import izip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
