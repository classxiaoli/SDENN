{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import numpy as np # Matrix and vector computation package\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib inline\n",
    "# Set the seed of the numpy random number generator so that the tutorial is reproducable\n",
    "np.random.seed(seed=1)\n",
    "from sklearn import datasets, cross_validation, metrics # data and evaluation utils\n",
    "from matplotlib.colors import colorConverter, ListedColormap # some plotting functions\n",
    "import itertools\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "\n",
    "T = np.zeros((digits.target.shape[0],10))\n",
    "T[np.arange(len(T)), digits.target] += 1\n",
    "\n",
    "X_train, X_test, T_train, T_test = cross_validation.train_test_split(\n",
    "    digits.data, T, test_size=0.4)\n",
    "X_validation, X_test, T_validation, T_test = cross_validation.train_test_split(\n",
    "    X_test, T_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x100 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 1), dpi=100)\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(1,10,i+1)\n",
    "    ax.matshow(digits.images[i], cmap='binary') \n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the non-linear functions used\n",
    "def logistic(z): \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_deriv(y):  # Derivative of logistic function\n",
    "    return np.multiply(y, (1 - y))\n",
    "    \n",
    "def softmax(z): \n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layers used in this model\n",
    "class Layer(object):\n",
    "    \"\"\"Base class for the different layers.\n",
    "    Defines base methods and documentation of methods.\"\"\"\n",
    "    \n",
    "    def get_params_iter(self):\n",
    "        \"\"\"Return an iterator over the parameters (if any).\n",
    "        The iterator has the same order as get_params_grad.\n",
    "        The elements returned by the iterator are editable in-place.\"\"\"\n",
    "        return []\n",
    "    \n",
    "    def get_params_grad(self, X, output_grad):\n",
    "        \"\"\"Return a list of gradients over the parameters.\n",
    "        The list has the same order as the get_params_iter iterator.\n",
    "        X is the input.\n",
    "        output_grad is the gradient at the output of this layer.\n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    # def add高斯noise *****\n",
    "    def add_gaosi_noise(self,sigm):\n",
    "        pass\n",
    "    \n",
    "    # def remove高斯noise *****\n",
    "    def remove_gaosi_noise(self):\n",
    "        pass\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step linear transformation.\n",
    "        X is the input.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad=None, T=None):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\n",
    "        Y is the pre-computed output of this layer (not needed in this case).\n",
    "        output_grad is the gradient at the output of this layer \n",
    "         (gradient at input of next layer).\n",
    "        Output layer uses targets T to compute the gradient based on the \n",
    "         output error instead of output_grad\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(Layer):\n",
    "    \"\"\"The linear layer performs a linear transformation to its input.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_in, n_out):\n",
    "        \"\"\"Initialize hidden layer parameters.\n",
    "        n_in is the number of input variables.\n",
    "        n_out is the number of output variables.\"\"\"\n",
    "        self.W = np.random.randn(n_in, n_out) * 0.1\n",
    "        self.b = np.zeros(n_out)\n",
    "        \n",
    "        # 存下本层的结点数*****\n",
    "        self.nodenum = n_out\n",
    "        self.gaosi_noise = 0\n",
    "        self.curbs = 0\n",
    "        \n",
    "    # def add高斯noise *****\n",
    "    def add_gaosi_noise(self,sigm):\n",
    "        self.gaosi_noise = np.random.randn(self.curbs, self.nodenum) * sigm\n",
    "        \n",
    "    # def remove高斯noise *****\n",
    "    def remove_gaosi_noise(self):\n",
    "        self.gaosi_noise = 0\n",
    "        \n",
    "    def get_params_iter(self):\n",
    "        \"\"\"Return an iterator over the parameters.\"\"\"\n",
    "        return itertools.chain(np.nditer(self.W, op_flags=['readwrite']),\n",
    "                               np.nditer(self.b, op_flags=['readwrite']))\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step linear transformation.\"\"\"\n",
    "        return X.dot(self.W) + self.gaosi_noise + self.b\n",
    "        \n",
    "    def get_params_grad(self, X, output_grad):\n",
    "        \"\"\"Return a list of gradients over the parameters.\"\"\"\n",
    "        JW = X.T.dot(output_grad)\n",
    "        Jb = np.sum(output_grad, axis=0)\n",
    "#         print(\"-----------------------------\")\n",
    "#         print(\"X.shape : \" + str(X.shape))\n",
    "#         print(\"output_grad.shape : \" + str(output_grad.shape))\n",
    "        return [g for g in itertools.chain(np.nditer(JW), np.nditer(Jb))]\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        return output_grad.dot(self.W.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticLayer(Layer):\n",
    "    \"\"\"The logistic layer applies the logistic function to its inputs.\"\"\"\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return logistic(X)\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        return np.multiply(logistic_deriv(Y), output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxOutputLayer(Layer):\n",
    "    \"\"\"The softmax output layer computes the classification propabilities at the output.\"\"\"\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return softmax(X)\n",
    "    \n",
    "    def get_input_grad(self, Y, T):\n",
    "        \n",
    "#         print(\"--------- input_grad ----------\")\n",
    "#         print(\"T.shape : \" + str(T.shape))\n",
    "#         print(\"Y.shape : \" + str(Y.shape))\n",
    "#         print(\"Y.shape[0] : \" + str(Y.shape[0]))\n",
    "        \n",
    "        re = (Y - T) / Y.shape[0]  \n",
    "        return re\n",
    "    \n",
    "    def get_cost(self, Y, T):\n",
    "        multi = np.multiply(T, np.log(Y))\n",
    "#         print(\"--------- cost ----------\")\n",
    "#         print(\"T.shape : \" + str(T.shape))\n",
    "#         print(\"Y.shape : \" + str(Y.shape))\n",
    "#         print(\"Y.shape[0] : \" + str(Y.shape[0]))\n",
    "        return - multi.sum() / Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation : include gaosi noise\n",
    "def forward_step(input_samples, layers,bs,sigm):\n",
    "    activations = [input_samples] \n",
    "    X = input_samples\n",
    "    for layer in layers:\n",
    "        layer.curbs = bs\n",
    "        layer.add_gaosi_noise(sigm)\n",
    "        Y = layer.get_output(X)  \n",
    "        activations.append(Y)  \n",
    "        X = activations[-1]  \n",
    "    return activations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation : not include gaosi noise\n",
    "def forward_step2(input_samples, layers):\n",
    "    activations = [input_samples] # List of layer activations\n",
    "    X = input_samples\n",
    "    for layer in layers:\n",
    "        # remove gaosi noise\n",
    "        layer.remove_gaosi_noise()\n",
    "        Y = layer.get_output(X) \n",
    "        activations.append(Y) \n",
    "        X = activations[-1]  \n",
    "    return activations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the backward propagation step as a method\n",
    "def backward_step(activations, targets, layers):\n",
    "    param_grads = collections.deque() \n",
    "    output_grad = None \n",
    "    for layer in reversed(layers):   \n",
    "        Y = activations.pop() \n",
    "        if output_grad is None:\n",
    "            input_grad = layer.get_input_grad(Y, targets)\n",
    "        else: \n",
    "            input_grad = layer.get_input_grad(Y, output_grad)\n",
    "        X = activations[-1]\n",
    "        grads = layer.get_params_grad(X, output_grad)\n",
    "        param_grads.appendleft(grads)\n",
    "        output_grad = input_grad\n",
    "    return list(param_grads)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform gradient checking\n",
    "# nb_samples_gradientcheck = 10 # Test the gradients on a subset of the data\n",
    "# X_temp = X_train[0:nb_samples_gradientcheck,:]\n",
    "# T_temp = T_train[0:nb_samples_gradientcheck,:]\n",
    "# # Get the parameter gradients with backpropagation\n",
    "# activations = forward_step(X_temp, layers)\n",
    "# param_grads = backward_step(activations, T_temp, layers)\n",
    "\n",
    "# # Set the small change to compute the numerical gradient\n",
    "# eps = 0.0001\n",
    "# # Compute the numerical gradients of the parameters in all layers.\n",
    "# for idx in range(len(layers)):\n",
    "#     layer = layers[idx]\n",
    "#     layer_backprop_grads = param_grads[idx]\n",
    "#     # Compute the numerical gradient for each parameter in the layer\n",
    "#     for p_idx, param in enumerate(layer.layer_index()):\n",
    "#         grad_backprop = layer_backprop_grads[p_idx]\n",
    "#         # + eps\n",
    "#         param += eps\n",
    "#         plus_cost = layers[-1].get_cost(forward_step(X_temp, layers)[-1], T_temp)\n",
    "#         # - eps\n",
    "#         param -= 2 * eps\n",
    "#         min_cost = layers[-1].get_cost(forward_step(X_temp, layers)[-1], T_temp)\n",
    "#         # reset param value\n",
    "#         param += eps\n",
    "#         # calculate numerical gradient\n",
    "#         grad_num = (plus_cost - min_cost)/(2*eps)\n",
    "#         # Raise error if the numerical grade is not close to the backprop gradient\n",
    "#         if not np.isclose(grad_num, grad_backprop):\n",
    "#             raise ValueError('Numerical gradient of {:.6f} is not close to the backpropagation gradient of {:.6f}!'.format(float(grad_num), float(grad_backprop)))\n",
    "# print('No gradient errors found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample model to be trained on the data\n",
    "hidden_neurons_1 = 20  # Number of neurons in the first hidden-layer\n",
    "# hidden_neurons_2 = 10  # Number of neurons in the second hidden-layer\n",
    "# Create the model\n",
    "layers = [] # Define a list of layers\n",
    "# Add first hidden layer\n",
    "layers.append(LinearLayer(X_train.shape[1], hidden_neurons_1))\n",
    "layers.append(LogisticLayer())\n",
    "# Add second hidden layer\n",
    "# layers.append(LinearLayer(hidden_neurons_1, hidden_neurons_2))\n",
    "# layers.append(LogisticLayer())\n",
    "# Add output layer\n",
    "layers.append(LinearLayer(hidden_neurons_1, T_train.shape[1]))\n",
    "layers.append(SoftmaxOutputLayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "# Create the minibatches\n",
    "batch_size = 25  # Approximately 25 samples per batch\n",
    "nb_of_batches = X_train.shape[0] / batch_size  # Number of batches\n",
    "# Create batches (X,Y) from the training set\n",
    "XT_batches = zip(\n",
    "    np.array_split(X_train, nb_of_batches, axis=0),  # X samples\n",
    "    np.array_split(T_train, nb_of_batches, axis=0))  # Y targets\n",
    "\n",
    "print(len(XT_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a method to update the parameters\n",
    "def update_params(layers, param_grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Function to update the parameters of the given layers with the given gradients\n",
    "    by gradient descent with the given learning rate.\n",
    "    \"\"\"\n",
    "    for layer, layer_backprop_grads in zip(layers, param_grads):\n",
    "        for param, grad in itertools.izip(layer.get_params_iter(), layer_backprop_grads):\n",
    "            # The parameter returned by the iterator point to the memory space of\n",
    "            #  the original layer and can thus be modified inplace.\n",
    "            param -= learning_rate * grad  # Update each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 20\n",
      "epoch : 40\n",
      "epoches : 50\n"
     ]
    }
   ],
   "source": [
    "# Perform backpropagation\n",
    "# initalize some lists to store the cost for future analysis        \n",
    "minibatch_costs = []\n",
    "training_costs = []\n",
    "validation_costs = []\n",
    "\n",
    "sigm = 2\n",
    "max_nb_of_iterations = 50  # Train for a maximum of 300 iterations\n",
    "learning_rate = 0.1  # Gradient descent learning rate\n",
    "\n",
    "# Train for the maximum number of iterations\n",
    "for iteration in range(max_nb_of_iterations):\n",
    "    for X, T in XT_batches:  # For each minibatch sub-iteration\n",
    "        bs = len(X)\n",
    "        activations = forward_step(X, layers,bs,sigm)  # Get the activations\n",
    "        param_grads = backward_step(activations, T, layers)  # Get the gradients\n",
    "        update_params(layers, param_grads, learning_rate)  # Update the parameters\n",
    "        \n",
    "        # ***** get minibatch cost\n",
    "        activations2 = forward_step2(X, layers)  \n",
    "        minibatch_cost = layers[-1].get_cost(activations2[-1], T)  \n",
    "        minibatch_costs.append(minibatch_cost)\n",
    "        \n",
    "    # Get full training cost for future analysis (plots)\n",
    "    activations = forward_step2(X_train, layers)\n",
    "    train_cost = layers[-1].get_cost(activations[-1], T_train)\n",
    "    training_costs.append(train_cost)\n",
    "    # Get full validation cost\n",
    "    activations = forward_step2(X_validation, layers)\n",
    "    validation_cost = layers[-1].get_cost(activations[-1], T_validation)\n",
    "    validation_costs.append(validation_cost)\n",
    "    \n",
    "    if (iteration + 1) >= 30: # 保证至少跑完30个epoch\n",
    "        #grad没降时停止训练\n",
    "        if len(validation_costs) > 3:\n",
    "            if validation_costs[-1] >= validation_costs[-2] >= validation_costs[-3]:\n",
    "                break\n",
    "    if (iteration + 1) % 20 == 0 :\n",
    "        print(\"epoch : \" + str(iteration + 1))\n",
    "nb_of_iterations = iteration + 1  # The number of iterations that have been executed\n",
    "print(\"epoches : \" + str(nb_of_iterations))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEPCAYAAAC+35gCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xd8VFX6+PHPkxBClxaVIm0FRDCEjlIMioBir6xlhVVZWb/qFlldGyzu6hZF17WCupYfq4IKq6ILqIk0QZAqvQUIQQidEEpCnt8fZ5KZJJNkJpnMpDzv1+u+Mnfm3jvPnCT3mXPuPeeIqmKMMcYEIirSARhjjKk8LGkYY4wJmCUNY4wxAbOkYYwxJmCWNIwxxgTMkoYxxpiAhTVpiMg5IpIkIutEZI2IPOhnm0QROSwiKzzLk+GM0RhjTNFqhPn9soHfq+oyEakP/CAic1R1bYHt5qnqlWGOzRhjTAnCWtNQ1d2quszz+CiwDmgRzhiMMcaUXrhrGnlEpA3QDVjs5+ULRWQlkAY8pKpr/Ow/GhgNUKtWrR6tWrUqv2ArkZycHKKi7FIVWFn4srLwsrLw2rhx4z5VjQtmH4nEMCIiUg/4FviLqn5S4LUGQI6qZojIFcA/VbV9ccfr2LGjbtiwofwCrkSSk5NJTEyMdBgVgpWFl5WFl5WFl4j8oKo9g9kn7OlWRGKAj4EpBRMGgKoeUdUMz+MvgBgRaRrmMI0xxvgR7runBHgTWKeqE4vY5mzPdohIb1yM+8MXpTHGmKKE+5pGP+AOYLWIrPA89yjQCkBVXwNuBMaISDZwHBihNhSvMcZUCGFNGqo6H5AStnkJeCk8ERljQiUrK4vU1FROnDgR6VCKdcYZZ7Bu3bpIhxFWtWrVomXLlsTExJT5WBG7e8oYU7WkpqZSv3592rRpg6eFuUI6evQo9evXj3QYYaOq7N+/n9TUVNq2bVvm49l9Z8aYkDhx4gRNmjSp0AmjOhIRmjRpErIaoCUNY0zIWMKomEL5e7GkYYwxJmCWNIwxVcKhQ4d45ZVXSrXvFVdcwaFDh4rd5sknn+Srr74q1fHLYsaMGaxdW3B4vsixpGGMqRKKSxqnT58udt8vvviChg0bFrvNhAkTGDx4cKnjKy1LGsYYUw4eeeQRtmzZQkJCAmPHjiU5OZlBgwZx6623csEFFwBw7bXXMnDgQDp37sykSZPy9m3Tpg379u0jJSWFTp06cc8999C5c2eGDBnC8ePHARg5ciQfffRR3vbjxo2je/fuXHDBBaxfvx6A9PR0LrvsMrp3786vfvUrWrduzb59+/LFefr0aUaOHEmXLl244IILeP755wHYsmULw4YNo0ePHgwYMID169ezcOFCPv30U8aOHUtCQgJbtmwp93IskapW+qVDhw5qnKSkpEiHUGFYWXiFoyzWrl2b9xjKbynKtm3btHPnznnrSUlJWqdOHd26dWvec/v379cjR45oZmamdu7cWfft26eqqq1bt9b09HTdtm2bRkdH6/Lly1VV9aabbtL33ntPVVXvvPNOnTZtWt72L774oqqqvvzyy3rXXXepqup9992nTz/9tKqqfvnllwpoenp6vjiXLl2qgwcPzls/ePCgqqpecsklunHjRlVVXbRokQ4aNKjQ+5aF7+8nF7BUgzzfWj8NY0yV1bt373x9E1588UU+/vhjoqKi2LlzJ5s2baJJkyb59mnbti0JCQkA9OjRg5SUFL/Hvv766/O2+eQTN4ze/PnzmT59OgDDhg2jUaNGhfZr164dW7du5f7772f48OEMGTKEjIwMFi5cyE033ZS33cmTJ0v/wcuRJQ1jTJVVt27dvMfJycl89dVXfPXVV5x11lkkJib67bsQGxub9zg6Ojqveaqo7aKjo8nOzgZcy01JGjVqxMqVK5k1axYvv/wyU6dO5YUXXqBhw4asWLGixP0jza5pGGNCrjwbqIpSv359jh49WuTrhw8fplGjRtSpU4f169ezaNGikH/u/v37M3XqVABmz57NwYMHC22zb98+cnJyuOGGG3jqqadYtmwZDRo0oG3btkybNg1wyWflypUBfa5ws6RhjKkSmjRpQr9+/ejSpQtjx44t9PqwYcPIzs7mwgsv5IknnqBv374hj2HcuHHMnj2b7t278+WXX9KsWbNCQ5bs2rWLxMREEhISGDlyJM888wwAU6ZM4c0336Rr16507tyZ//73vwCMGDGCf/zjH3Tr1q1CXAiPyCRMoWaTMHnZBDNeVhZe4SiLdevW0alTp3J9j1Aoz7GnTp48SXR0NDVq1OC7775jzJgxFabJyd/vpzSTMNk1DWOMCZEdO3Zw8803k5OTQ82aNZk8eXKkQwo5SxrGGBMi7du3Z/ny5ZEOo1zZNQ1jjDEBs6RhjDEmYJY0jDHGBMyShjHGmIBZ0jDGVFv16tUDIC0tjRtvvNHvNomJiSxdurTY47zwwgtkZmbmrQcy1HqopaSk8J///Kfc38eShjGm2mvevHneCLalUTBpBDLUeqhZ0jDGmCA8/PDD+ebTGD9+PM899xwZGRlceumlecOYz5w5s9C+KSkpdOnSBYDjx48zYsQI4uPjueWWW/KNPTVmzBh69uxJ586dGTduHOAGQUxLS2PQoEEMGjQI8A61DjBx4kS6dOlCly5deOGFF/Ler6gh2H1NmzaNLl260LVrVwYOHAi4odXHjh1Lr169iI+P5/XXXwfc0PDz5s0jISEhb7j1chHssLgVcbGh0b1sOHAvKwuvcA+Nrqqq48YFPqTUPfcUPuA99+TfZty4Yt9/2bJlOnDgwLz1Tp066fbt2zUrK0sPHz6sqqrp6enatm1bzcnJUVXVunXrqmr+YdWfe+45HTVqlKqqrly5UqOjo3XJkiWq6oZWV1XNzs7Wiy++WFeuXKmq3qHVc+WuL126VLt06aIZGRl69OhRPf/883XZsmXFDsHuq0uXLpqamqqq3iHUX3/9dX3qqadUVfXEiRPao0cP3bp1qyYlJenw4cOLLJ9QDY1uNQ1jTJXQrVs39u7dS1paGitXrqRRo0a0atUKVeXRRx8lPj6ewYMHs3v3bvbs2VPkcebOncvtt98OQHx8PPHx8XmvTZ06le7du9OtWzfWrFlT4ox68+fP57rrrqNu3brUq1eP66+/nnnz5gGBDcHer18/Ro4cyeTJk/NmH5w9ezbvvvsuCQkJ9OnTh/3797Np06agyqosrEe4MabKuPHGG/noo4/46aefGDFiBOAGAkxPT+eHH34gJiaG1q1b+x0S3ZeIFHpu27ZtPPvssyxZsoRGjRoxcuTIEo+jxYztF8gQ7K+99hqLFy9m5syZJCQksGLFClSVf/3rXwwdOjTftsnJycXGEipW0zDGlI/x4wMf79xn6tU8kybl32b8+BLfcsSIEXzwwQd89NFHeXdDHT58mDPPPJOYmBiSkpLYsWNHsccYOHAgU6ZMAeDHH39k1apVABw5coS6detyxhlnsGfPHr788su8fYoavnzgwIHMmDGDzMxMjh07xvTp0xkwYECJnyPXli1b6NOnDxMmTKBp06bs3LmToUOH8uqrr5KVlQXAxo0bOXbsWNiGULeahjGmyujcuTNHjx6lRYsWNGvWDIDbbruNq666ip49e5KQkECHDh2KPcaYMWMYNWoU8fHxJCQk0Lt3bwC6du1Kt27d6Ny5M+3ataNfv355+4wePZrLL7+cZs2akZSUlPd89+7dGTlyZN4x7r77brp161bkbIAFjR07lk2bNqGqXHrppXTt2pX4+HhSUlLo3r07qkpcXBwzZswgPj6eGjVq0LVrV0aOHMlvf/vbYIouYDY0ehVjw4F7WVl42dDoXuU5NHpFFqqh0a15yhhjTMAsaRhjjAmYJQ1jTMhUhebuqiiUvxdLGsaYkKhVqxb79++3xFHBqCr79++nVq1aITme3T1ljAmJli1bkpqaSnp6eqRDKdaJEydCdgKtLGrVqkXLli1DcixLGsaYkIiJiaFt27aRDqNEycnJdOvWLdJhVFrWPGWMMSZgYU0aInKOiCSJyDoRWSMiD/rZRkTkRRHZLCKrRKR7OGM0xhhTtHA3T2UDv1fVZSJSH/hBROaoqu+oX5cD7T1LH+BVz09jjDERFtaahqruVtVlnsdHgXVAiwKbXQO86xm5dxHQUESahTNOY4wx/kXsQriItAG6AYsLvNQC2Omznup5bneB/UcDowHi4uLCNsJjRZeRkWFl4WFl4WVl4WVlUTYRSRoiUg/4GPiNqh4p+LKfXQrd+K2qk4BJ4MaesjGGHBtvycvKwsvKwsvKomzCfveUiMTgEsYUVf3EzyapwDk+6y2BtHDEZowxpnjhvntKgDeBdao6sYjNPgV+4bmLqi9wWFV3F7GtMcaYMAp381Q/4A5gtYis8Dz3KNAKQFVfA74ArgA2A5nAqDDHaIwxpghhTRqqOh//1yx8t1HgvvBEZIwxJhjWI9wYY0zALGkYY4wJmCUNY4wxAbOkYYwxJmCWNIwxxgTMkoYxxpiAWdIwxhgTMEsaxhhjAmZJwxhjTMCqdNLIyYGdOyEzM9KRGGNM1VBlk8add0K9etCqFSxcGOlojDGmaqiySSMqCo4fd483boxsLMYYU1VU2aRxZdZ0ZnAN6+nI2R/+M9LhGGNMlRCx6V7L27k1d9CVTwHYtvXHCEdjjDFVQ5WtaTTo1THvceN9GyIYiTHGVB1VNmk0S/QmjdYnNnDqVASDMcaYKqLKJo1aHVpxklgAzmIvKSsORTgiY4yp/Kps0iA6mrR67fNWf/rWmqiMMaasqm7SAA6d5W2iyvjBkoYxxpRVlU4a2e28SUPXW9IwxpiyqtJJIzbemzTq7rKkYYwxZVWlk0aTi7xJ46xDljSMMaasqnTSOPtib9Jom72JIwdPRzAaY4yp/Kp00ohu0pBHmr9Lf+bRklQ2bY2OdEjGGFOpVemkAbCh9x0soD/7aWoDFxpjTBlV+aTRoYP38Qa7rGGMMWVSrZKG1TSMMaZsqk3SaMhBTq1cF9lgjDGmkquyQ6PnOq9eKnvozpmks2tdC1RTEYl0VMYYUzlV+ZpG085n0YiDALTQXezdmhHhiIwxpvKq8klDasawq9bP8tZ3JdmFDWOMKa0qnzQA9jf1dvI7tNhuoTLGmNKqFknjZGtv0ji91pKGMcaUVrVIGjU6e5NG7HZLGsYYU1phTRoi8paI7BWRH4t4PVFEDovICs/yZCje94ze3qTR1OYLN8aYUgv3LbdvAy8B7xazzTxVvTKUb9riEm/SaHVyI9lZSo0Yu+/WGGOCFdaahqrOBQ6E8z0B6rVpykFp5B5zjNTFu8IdgjHGVAkVsXPfhSKyEkgDHlLVNf42EpHRwGiAuLg4kpOTiz1oo9rn0ihzCQCL3vmSlOz2xW5fWWVkZJRYFtWFlYWXlYWXlUXZVLSksQxoraoZInIFMAPwe3ZX1UnAJICOHTtqYmJisQde2PJ8Tm/8gW20pY40o6TtK6vk5OQq+9mCZWXhZWXhZWVRNhXq7ilVPaKqGZ7HXwAxItI0FMdefvtE6pBJezYzKyakl0yMMabaqFBJQ0TOFnEjQ4lIb1x8+0Nx7FYJjTlFLGCj3RpjTGmFtXlKRN4HEoGmIpIKjANiAFT1NeBGYIyIZAPHgRGqqqF4b5tXwxhjyi6sSUNVf17C6y/hbskNubZtIToaTp+GnTshMxPq1CmPdzLGmKqrol0ILzc1a0K/ltupu30NHdnA9vk30mnIOZEOyxhjKpVqkzQAns0cQy++BOC7WeeAJQ1jjAlKhboQXt4yW3p7hp9cZRc2jDEmWNUqach53qQRs9WShjHGBKtaJY16PbxJo+FeSxrGGBOsapU0zr7YmzRaHtsAobmb1xhjqo2gkoaI1BCR2ALPDRGR34hI99CGFnpnd2vGUeoBcIYe5sD6vRGOyBhjKpdgaxofAq/mrojIA8D/gGeARSJSocfniIoWdtT21jbSkqyJyhhjghFs0ugLfOGzPhZ4TlVrA28Aj4UqsPJyIM6bNI4ssaRhjDHBCDZpNAF+AhCRC4DmwGue16YB54cutPJxqo3PfOHrLGkYY0wwgk0ae4A2nsfDgO2qusWzXhvICVFc5Samizdp1N5hScMYY4IRbI/wacDfRKQrMIr840R1AzaFKrDy0rBfZ75/pRcb6EhKVH96RjogY4ypRIJNGo8AR4BeuAviT/u81gN3obxCazmsC034HoDaB+CxHIiqVjceG2NM6QWVNFQ1G5hQxGvXhySicta4McTFQXo6HD8OW7ZA+6o586sxxoRcsP00zhSRtj7rIiKjReQFEbkq9OGVj169vI8XLoxcHMYYU9kE2zDzNvBbn/U/Aa/gLopPF5GRoQmrfPXr5328YL71CjfGmEAFmzS6A98AiEgUMAZ4VFXPA/4C/Ca04ZWPfn1P8yAvMJ1r+dPbrSErK9IhGWNMpRDshfAz8M7Z3QNoDEzxrH8D/D5EcZWrXn2jacU/aUsKZMORr5fQYNhFkQ7LGGMqvGBrGql4O/ANB9ar6i7P+hnAiVAFVp7q1IHVTQflre9+PzlywRhjTCUSbNJ4C/i7iEwD/gBM8nmtL7AuVIGVt6M9vEkjem5SBCMxxpjKI6ikoarPAPfjhhK5H3jR5+XGuPGnKoUzrknMe9xyxwI4eTJywRhjTCURdLc2VX1XVe9X1TdVvRNSqOq9qvpOaMMrP92vOYfN/AyAWjnHyV74fYQjMsaYii/opOGZU+MWEfmXiEzx/LxZRIK9qB5RzZvD0nreJqqfPkyOXDDGGFNJBN25D1gKvI+7EN7O8/MDYImIxIU8wnK0P96bNPRru65hjDElCbamMRE3PHofVW2nqheqajugj+f5iaEOsDzVHZ6Y9/isrQvhRKW4+csYYyIm2KRxBfCwqi7xfdKz/kdcraPS6Da8ORvoAEDNnJPod4siHJExxlRswSaNWOBoEa8dBWqWLZzw6tIFFsS4JqqT1GT/91tK2MMYY6q3YJPGIuBhEanr+6Rn/WHP65VGdDQs7nkfl/A1DTnErJZ3RTokY4yp0IK94+n3QBKwU0Rm42byOxMYCgiQGNLowqD50AuY9J17vGAB3HZbZOMxxpiKLNjOfSuADrie4HHAZbik8RrQXlVXhjzCcuY74q0Nk26MMcULum+FqqbjZvCrEvr0cTP35eTA6tVw5Ag0aBDpqIwxpmIqMWmIyBIg4EknVLV3mSIKs/r1oWtX2LL8MANz5pL2yDYavPJApMMyxpgKKZCaxhqCSBqV0aU9DvH98qbU4DTZr8fAP+6CunVL3tEYY6qZEpOGqo4MQxwR1f2Shqx7oxMX8CM1crLcFfEhQyIdljHGVDhBjz1VFfXrB0l4hxTJ+SY5csEYY0wFFtakISJvicheEfmxiNdFRF4Ukc0iskpEuocjrlatYHWTxLz141/YOFTGGONPuGsabwPDinn9cqC9ZxkNvBqGmADI6X8xOQgAtdcsgaNFdXw3xpjqK6xJQ1XnAgeK2eQa4F11FgENRaRZOGLrekkTVhEPQFTOaZg/Pxxva4wxlUpFmwOjBbDTZz3V89zughuKyGhcbYS4uDiSk5PL9MaxsfVIYhAJuP6JO955h621a5fpmJGQkZFR5rKoKqwsvKwsvKwsyqaiJQ3x85zf231VdRKeOco7duyoiYmJZXrj/v3h1gd28dtTLwBw9pqNtCrjMSMhOTmZspZFVWFl4WVl4WVlUTYV7e6pVOAcn/WWQFo43rhGDTjeJ5EsTx6t+eNy2F2ogmOMMdVaRUsanwK/8NxF1Rc4rKphO3MnXHwG8xjgfeKLL8L11sYYUymEtXlKRN7HjYTbVERSgXFADICqvgZ8gZvoaTOQCYwKZ3z9+sFUbuYAjVl1zpVMuLpSzSlljDHlLqxJQ1V/XsLrCtwXpnAK6dcPro65l9ez7oWdMCYbwnLrljHGVBIVrXkqourXdxfEc82eHblYjDGmIrKkUcAwn66H//tf5OIwxpiKyJJGAb5JY86sHE4vWRa5YIwxpoKxpFHABRdAs2bwIvez5mAzonv3gI0bIx2WMcZUCJY0ChBxtY1z2MlZ7HVPzpwZ2aCMMaaCsKThx7BhMBOf220taRhjDGBJw6/Bg+F/ckXeus6d6yYPN8aYas6Shh+NG8M5fVuwnAQAJCsL5syJcFTGGBN5ljSKYE1UxhhTmCWNIhRMGvrFF5CTE8GIjDEm8ixpFKFHD9jSuDfpNAVA9uyBZdZnwxhTvVnSKEJ0NAweGs2XXO590pqojDHVnCWNYth1DWOMyc+SRjGGDIFZDOUkNfmOCzlx+XWgficSNMaYaqGiTfdaoZx9NrTr1pC45ekcpQHTu8G1/iakNcaYasJqGiUYNgyO0gCwUW+NMcaSRgkKDpVurVPGmOrMkkYJLrzQTc4EsH07bNgAnD4d0ZiMMSZSLGmUICbGjUV1Lpv4C4/S5NKucO21kQ7LGGMiwpJGAIYNg9oc5xH+SlzaKvj8c/j660iHZYwxYWdJIwBDh8Jq4nmbkd4nf/97a6YyxlQ7ljQC0Lo1dOoET/AUx6jjnly5Et59N7KBGWNMmFnSCNCwYZBGC57lIe+Tjz8Ox45FLihjjAkzSxoBuvtuNxXsPxjLbs52T6alwXPPRTYwY4wJI0saATr/fLjpJjhGPR7nz94X/v532L07coEZY0wYWdIIwhNPuJ9vM5JVXOBWjh3zvmCMMVWcJY0gdOkCN94IOUTzEM96X3jrLVi1KnKBGWNMmFjSCNKTT7qfcxjCl3jGGImLg9TUyAVljDFhYkkjSBdcANdf7x4/xLNM6/gYbN4MV1wR2cCMMSYMLGmUQm5tYy2duXnDn1m1rX5kAzLGmDCxpFEKXQsMPzVhQuRiMcaYcLKkUUq5tQ2Ajz+G1as9K2+/DR98EImQjDGm3FnSKKVu3eDqq73rfx1/AkaPhlGj4K674McfIxecMcaUE0saZeBb2/jskyxOfjXXrWRmwg03wJEjkQnMGGPKSdiThogME5ENIrJZRB7x8/pIEUkXkRWe5e5wxxioHj3gqqvc46PU57GOH0Mdz4CGGze6WodN9WeMqULCmjREJBp4GbgcOB/4uYic72fTD1U1wbO8Ec4YgzVunPfxxFmd2fgHn3A/+cTGpjLGVCnhrmn0Bjar6lZVPQV8AFwT5hhCqkcP77UNVbhyys/JGnO/d4NHHoFvv41McMYYE2KiYWw+EZEbgWGqerdn/Q6gj6r+n882I4FngHRgI/BbVd3p51ijgdEAcXFxPaZOnVr+H6AIe/bE8stf9iIzswYAI67fwmsbruGMNWsAONWoEUsnT+ZUkyblHktGRgb16tUr9/epDKwsvKwsvKwsvAYNGvSDqvYMaidVDdsC3AS84bN+B/CvAts0AWI9j+8FvinpuB06dNBImzxZ1dU1VEVUF320UzUuzvtkz56qhw6VexxJSUnl/h6VhZWFl5WFl5WFF7BUgzyPh7t5KhU4x2e9JZDmu4Gq7lfVk57VyUCPMMVWJnfd5aaFBZclbnu4Jcf//QFEeYp46VK4/fbIBWiMMSEQ7qSxBGgvIm1FpCYwAvjUdwMRaeazejWwLozxlZoIvPEGnHGGW9+yBR6ZfQm8+qp7omlT+POfiz6Ar88+cxdKnnjC5iE3xlQoNcL5ZqqaLSL/B8wCooG3VHWNiEzAVZM+BR4QkauBbOAAMDKcMZZFy5bwwgvuTluAF1+E65NHc/EkgYsugs6diz9Aejo8+CC8/75bP3IEDh2CMFwLMcaYQIQ1aQCo6hfAFwWee9Ln8R+BP4Y7rlC580746COYOdOtjxoFq1bdQ7HX3VTd0CMPPAD79rnnhg6F996zhGGMqVCsR3iIicCkSdCwoVvftg0eftjPhsuXwy9/CSkpcM01cOut3oQBcNZZ3ushxhhTQdhZqRw0b+6apnK98gp8/bXPBkuWwCWXwL//DT/7mbuGkeucc+CLL+Cdd6yWYYypcCxplJPbb88/oOHNN8PKlZ6Vzz931yoAcnK8G40Z4wY6vPzy/AfLzoann4b168s1ZmOMKYkljXIiAq+/7q0sHDgAgwd7Br8dP95d8M517rmu1/grr0CDBvkPtG4d9O0Ljz3mmrPsbipjTARZ0ihHZ58N//uf9zbcffvg0kth3XqB5593E3G89RasWgUDB/o/yKlT3irKd9+527OMMSZCLGmUs549YdYsqO+ZEXbvXnc5Y8NGcZONjxoFtWsXfYCuXV1/jVyPPw4bNpRv0MYYUwRLGmHQp4+rceTedvvTTy5xbN4c4AH++EdISHCPT5ywZipjTMRY0giTiy5yN0XlTreRlgaDBsHWrQHsHBPjppGt4elWs3ChO+BVV8FNN+Ufn90YY8pR2Dv3VWcDBrgbp4YPh+PHITUV+veHIUOgXTto29b9bNfOXQ8R8dm5a1fXNDV+vFv//nvva/37w5/+FM6PYoyppqymEWaDBsGnn0KtWm59927XJWPcOPjFL9z5v3lzqFsXRo6EY8d8dn70UVfDKMjfNZGZM2HCBHe7bijt2OEmlho0yN1H7Nsh0RhT5VlNIwIGD4YZM+CWW+DwYf/bHD/ukklamuv7FxuLa6ZKSnK9yY8edRudOFG4E+Du3S7j7NsHc+bAlCnQqlXpA05NdWOjfPghLFrkff7MM12ATZuW/tiVxeefu34yv/qV964GY6ohSxoRMnSoG0FkyRI31MjWrd5l2zbXrwPcOf/nP4epUz2XNGrWdFfWi/PPf3prAPPnQ3y8G9vk5pvzb5edDdHR+dvB9u1zCeLAAbcsXuyO4c9337m2tKrugw/cLwHgv/913ftr1oxsTMZEiCWNCGrYEC67zP9rTz0FT3qGcZw+3c3X8e9/Bzgc1V/+4m7VGj/e3WV1+LCr1jz9tOv3cfiwW44dg8zM/M1be/e6nulFiY52VaXrroO4uEA/auV24YVuLLA9e1wCfeABeO21SEdlTETYNY0K6vHH4Xe/866/+67rRB7Q7LzR0e4A8+ZBmzbe51eudD3M09K8F0sKto81alT4eFFRLlFMmuTuF/7f//w30xTV1lYap0/D6tVukpLf/Q5efhn27w/d8YPRujVn3tGSAAAW9klEQVTcd593/fXXA0sac+a42t3zz7tmRGOqAKtpVFAi8OyzbkqNN95wz730kutdHuhcTlx4IaxYAb/+NfznP/7f5MgRd6tWrsaN4Z573M9GjaBZMxg2zF2/KM7cua728frrcOONAQboY+9eWLDANYctXuxmOszIyL9NSgr84x/BHzsUHn/cJdzcuU7uv9/NjzJgQOFtVeGvf3VDv6jCtGmuJ//TT7tmLhu92FRmwc4PWxGXijBHeHnJzla95RbvVOOg+re/Fb19kfMfb9qkOm+e6urVqjt2qB4+rHr6dGiCnDVLtWZNF1xsrOr8+YHtl5Oj+sknqldeqRodnf9D+lvWrCl8jG3b3NzrR4+qHjumeuKE6qlTqqdP67xPP1VdulT1ww9VFywI7jOlpRV+7tgx1W7dvPHExalu355/m4yMwr8w36VHD9VvvgkulrI6eFDnzpwZ3D4ZGarffqs6Z4532bmz8HY5OaGJMYzy/keOH49oHBGVk6N66FCp5giP+Ak/FEtVThqqqidPql5xRf5zz+9+p7plS+Fti0wa5WnfPtUOHbzBNW6sunChO5mXdFLp3bvoE2yzZqrXXqv6wAOqd99deN9du1SjokpONqD6y18W3n/BAtUHH1Q9eDD/81995ZLfCy8Ujn/7dpcsco/bvbtLJrnuvTf/+/bsmX/73OXqq1WzsgIr32BlZ7vyf/JJ1V69VEU0R0T1uuvc88XZu1f1iSdUGzUqHPNrrxXe/tprVc85R7VfP9URI1T/8AfVV15xXyQ2by6/z1iSnBzV9etdzA88kO+lpKQk98Widm3VTp1Ub7tNdeJE1aQk1cWL3e9/xgzV995zn+X//b+IfIRykZWl+v77qgkJqldeaUmjKsvMVL344sL/x5de6v4Gcr80RSRpqLoMduaZhQOMjVVt3dolh6uvdiclX6+95t22b1/Vhx5SnTbN1YZKSjh//WtgCQNc4fnKzvbWGuLiVN94w9W8duxQbdrUu9+f/1z4fefOVa1Rw7vNrbd6Y92zR7VVK/f8r3/tTk6HD6s++qhqrVrefW6/vbQl7d++fapvv+1O3I0bF10OtWqpHjhQeP+UFNX773cn0qL29Zc04uOLL/foaNV27VQvu8x/DXTNGtXUVFdDDIUVK1ziPuus/HH41JKSkpLcdoH+7cTHF36fpUtVhwxxCWnCBPfl47bb3HPdu7u/gXr1VLt0cX9DFaVG9t//5vtsljSquMOHVS+6yP/fdePG7u/2zTe/j1yA339f/EkHVI8cyb/PoUOqjz3mms+C9ZvfqDZpolq/vmrduu6EWLOmO1GJaHZsrGrnzqpXXeX+sX1Nn144tp49XfNR7vrZZ/tvplJVffXV/PtOnux9bfly1UmTCu+zY4fqL37hyiglpfDr//636oYNwZfDt9/mT2IFF9/a2L335t93zRqXwPw1D7Zu7b6V5C6ffVb4vf3VSIpa/DXL5f69REWptm/vakNPPKH6wQeqP/7okm5Jjh9Xffdd1QsvLPq9fWoLSUlJ7otJoLXUtm0Lv+cbbwT+uQt+YSmrAwdUV6505bNunfub2bzZ/VyzRnXVKtVly1SXLHGv+8rOduUMqrVrW9KoDrKy3GWAK64o+m++a1fVv//dnaPCbsEC922rXTvVOnXyB1a7dli/cRVb68rJUZ061TWtFPUNee7c4t9g9Gi37Q03uGsqgdqzp/BzaWneE3f//qrPP+9ObPPmuYSae/ycnMLXojIzVRs0yB9/s2aqo0a5z3jggH7/1ltuvWBS8nfyS0hwJ+1AmpaOH3fxffONq+lMmKA6cqTqgAGqzZvnP27B6z/HjpV8wo2JUT3vPNVhw1T378+//+bNqmPHui8O/vZt2NDVbidOVN26NW+3vL+LY8dcc91LL7nmy5493TJokPuiceut7nc8blzhz/3QQ4EnjVdfLbz/5MmqjzyiOmWKu87oLzmmpLgaTUHvvRf4ew8dWnj/KVNc02V6uiWN6mbHDvc/2rq1/78XEfcl5/XXC/+/hc3Ro+6kMn++arAXY8sooKa6Y8fcN9vY2PyFN3FiyfuePKn61luhSYSBNLXVreuSw5dfFt7/rrtU+/Rxx1mxolBMRZbFiRPek/ugQe5aRCgT+7Fj7hvxjBnuW66vvXtd883ZZ7s/1uI+e1RU4ROrvxsOYmJcE11ycuH38whJE25Kivv29vTTqn/8o+pzz6m+8477G1+82CWpvXtd8k1Pz79vTo77UlUw7vh418Q1apSr3YBr1i3o7bcDTxqXXlrsxyhN0hC3X+XWsWNH3VCN55jIyXGdlN94A2bMOM2pU9GFtomJcRMEnj7tOoKfPp1/yf0zKPjnEBvrRiBp06bw0q6dd+Ddiig5OZnExMTANt66Ff7wBzf5yejR7n7nfCNGlrPkZHdb7ueflzzs/a23uqFhfGVluV9ykYcvpiw++8x1XuzdO6iQQ+r4cTdMy48/5l927HCvt2oF27fn3+fbbyH3M7Vu7foO/fKX7rMUI6i/i/KwZEngZR0V5UZmyJ3JDdyoBI8/7v5OcnK8P6Oi3D9kjRqur1aNGtCrF7z6apGHF5EfVLVnMOFb0qhiZs6cR3r6AP7zH5dIfKcgD7XmzWHyZLjiivJ7j7Io1clBNbzJoqCffnLDlqxf7x77LidPum2aNXMn0GKSREERP1GW1pEjblydI0cK94lRdX2Qhg+Hyy93J8oARLwsDh50X05Wr3azdq5eXTghgptHYcAANw10OQ3XU5qkUYG/J5rSqFv3NMOHu/EKd+92Y1ZNmeK+3IRaWpr7f33kETfsSUWudQQskgkDXEfL3/ym8POqrsf9gQPQokVQCaNSa9DATQvgj0ix36IrrEaNYMQIt+Q6fNjVrFatckP79O3ragkVcIyzqvBvborQrJkbeuTBB2HXLjh0yFtrjY72Po6Kyt9J2fe8mZHhvgSlpORfVq1yX5jAdX5euNB1lm7ePDSx5+S48RCnT4dvvnHnjT//2Z0vqyURN1hZw4aRjsSUhzPOgH793FLBWdKoJlq0KN0JNy7OTQ5VUHo63HGHq2WDG0UkIcGNVjJ4cOHt9+5180bt2OGanHPjadbMW0PJynIjv0+f7oaO/+kn7/7Ll7vROJ580n0RD+cXsI0b3fBXW7bA2LFuWvdIV0iMiRRLGqZU4uLc9LXPPONO5Dk5LpEMGeLWL7vMDSH1/ffuZ0qK/+OIuBaZ5s3dSfnQoaLf89gxePhhN9rvv/7lPzmFUnY2TJzoPk/u5YS77nJNfpMnwznnlO/7G1MRWdIwpRYV5cbk69fPjcP300+u6f1Pfwp89llVd+1l9+7Cr8XFwbXXuubd556DtWvd8+vXu6R0ww3upF7c/FKqrok4I8O7HDsGLVsWv9+qVe5GnB9+KPzarFlurMKJE10S8VfrOHHC3Yjwww/urrXhw/PfAGNMZWVJw5RZYqJrPrr1Vte85E9sLHTvDp06uXmeUlPddZa9e/Pf5tuqFVx/vRswt18/7w0xd9zhRvkdN85NWgjw8cfuDtUzz3Q1nYJLZmZ/TpwofBtxrjZt3Ky1iYluadXKTTfy9NNuShLfmXK7d3dzX732mjve0aNuMOBp01yto1UrN3L7zJnujshZs/JP1RsTA5de6j7XNdeUeFeoMRVXsB07KuJSXTv3+ROxsafU9aUaP951xD3vPNU771R9+WXXqbWo0SBOnnT9pBYscKMflNSvLC1N9Y47Au/bFMzSrp13hIXcJTZW9ZlnvJ2j588vvE39+q4DdCAD9eZ2uuzXT/XZZ92oD+HoJB/Jv4uKxsrCC+vcZyJ+D3qYzJ/vprRYsaLkbWvXdhMZ5i6xsa6pKzOz+P0uugjefBPOOy//85mZ7jrHxIlF12IAOnRwNZklS2DZsqK3a97cXQu67DJ3naakqUuCkZkJGzbAsmVLGDWql03lQfX5HwmEde4z1eofIvd6SHa297Zh32XRonlcfvkAv32+Tp1y8zwlJ7smtQULXKdkcH2qnnnGTdZXXH+xhQvdnVQbN7p1EdeEde21rgnKN9ls3+7uCJs+3U2oWFyny4QE6NbN3SGW28HXd6lTxyW/+vXz/wSXINaudfNFrVuX/waEs85yHTGvvNIlqIITL5465ZLbvHluWbvWvVejRt45uXIf16/vmtz8xde0qbvjrmXLitV3JyvL/R4WLlzMLbf0ITY20hFFniUNU62SRkmCKYtTp1yNYO1aGDq0+Ivkvo4fd1PxxsS4E7LvJIhFSU+HTz+FL790F8uLu2OsvMTEwMUXu47Uhw65JLF4sTdxhkKNGq4c27Z1S5s2ULeue+/chJP7uE4dd/t18+YuuZUm2Zw65a4rHTjgvkxs2uQSeu7Pbdu816lEXFL72c/ccu65rtN17douuRRcVN3vtmVLd9dcXFzhGyAOH3ZJe8MGd7PGli3QpAkMHOg6dpelD9O+fe6YsbHQpYuLsyz273e3yV9/vSWNas+ShldlKIvsbFfjmTMHZs+GRYvyX4Avq+hod1Lcu/cUhw5VvN7F/kRFucTRvLnry1OvniuTgsupUy7hHTjgToK+Nx6Ut5o1XQJp2dIljw0b8vcr8ufcc10Cufhi1/RZr17+seByP9e+fbBmjVvWrnU/9+71HicqytViExLyL02bFt1/6NAhlySSktyyalVu06oljWqvMpwow6UylsWRI+6fO7fZzfdkkp3tvvVmZrq7tzIy8v/MynInpk6d3HL++W49Nha+/jqZevUS+fxzd4fX8uX+379tW/eteMAA19R2+rQ7KR886P158KB7v4Kx5Z7Id+923+pLOolGQosWkJ19gvT0WuU6LlskREUVbrasX9/VgFasKKpJtBKMPSUiw4B/AtHAG6r61wKvxwLvAj2A/cAtqpoS7jiNiYQGDdw1h1CLjnZJoE8fN07Yrl2uc+bcue49cxNFKIdpycx011S2bXPLzp2u/0pu8vNdjh51yWbXLtd8V9rP2LixaxJq2tTVsDp0gPbt3c9zz3XNY8nJi7jookRSUlwT0ubN7mdKikuEuU1mvktOjhtrbedOd7u4vybF2Fj3Xh07uprAuee6zz13rqtBnjhR+rKsXdsdMzPTNbX5+66fk+O+dBw5ElhZ9erl4gpWWJOGiEQDLwOXAanAEhH5VFXX+mx2F3BQVc8VkRHA34BbwhmnMVVdixaun8k995Tfe9Sp42o7558f3H65tZW0NJdETp70f8G9Rg3XYTI3UTRoEPjwLjVrukTSoUPwnwtc7S411SWRnByXLFq3LvrGiZMn3TWzuXPdknvXn+9YcLmfqW5dlyA6d/Yubdp4x4fLyHAD465Y4V3WrCm+eS4qyvU1GjTILf37u1pIaYbDCXdNozewWVW3AojIB8A1gG/SuAYY73n8EfCSiIhWhXY0Y0yJatZ0J+DWrSMdSdHq1XMn9oK3YxclNtadqPv3h0cfLft7X3ihW3xlZXlHPTh61LsA9OwZurEuw500WgA7fdZTgT5FbaOq2SJyGGgC7PPdSERGA6M9qydF5MdyibjyaUqBsqrGrCy8rCy8rCy8Oga7Q7iThr/KUMEaRCDboKqTgEkAIrI02Is5VZWVhZeVhZeVhZeVhZeILA12n3D3D00FfMcGbQmkFbWNiNQAzgAOhCU6Y4wxxQp30lgCtBeRtiJSExgBfFpgm0+BOz2PbwS+sesZxhhTMYS1ecpzjeL/gFm4W27fUtU1IjIBN3DWp8CbwHsishlXwxhR9BHzTCq3oCsfKwsvKwsvKwsvKwuvoMuiSnTuM8YYEx425qUxxpiAWdIwxhgTsEqfNERkmIhsEJHNIvJIpOMJJxF5S0T2+vZREZHGIjJHRDZ5fjaKZIzhIiLniEiSiKwTkTUi8qDn+WpXHiJSS0S+F5GVnrL4k+f5tiKy2FMWH3puRqnyRCRaRJaLyOee9epaDikislpEVuTealua/49KnTR8hiW5HDgf+LmIBDloQaX2NjCswHOPAF+ranvga896dZAN/F5VOwF9gfs8fwvVsTxOApeoalcgARgmIn1xQ/I87ymLg7ghe6qDB4F1PuvVtRwABqlqgk8/laD/Pyp10sBnWBJVPQXkDktSLajqXAr3YbkGeMfz+B3g2rAGFSGqultVl3keH8WdJFpQDcvDM5Nnhmc1xrMocAluaB6oJmUhIi2B4cAbnnWhGpZDMYL+/6jsScPfsCQhHKezUjpLVXeDO5ECIZw8tHIQkTZAN2Ax1bQ8PE0yK4C9wBxgC3BIVXNn66gu/ysvAH8AcgcGb0L1LAdwXxxmi8gPnmGYoBT/HxVoMsZSCWjIEVN9iEg94GPgN6p6REozjGcVoKqngQQRaQhMBzr52yy8UYWXiFwJ7FXVH0QkMfdpP5tW6XLw0U9V00TkTGCOiKwvzUEqe00jkGFJqps9ItIMwPNzbwnbVxkiEoNLGFNU9RPP09W2PABU9RCQjLvO09AzNA9Uj/+VfsDVIpKCa7q+BFfzqG7lAICqpnl+7sV9kehNKf4/KnvSCGRYkurGdxiWO4H/RjCWsPG0Vb8JrFPViT4vVbvyEJE4Tw0DEakNDMZd40nCDc0D1aAsVPWPqtpSVdvgzg3fqOptVLNyABCRuiJSP/cxMAT4kVL8f1T6HuEicgXu20PusCR/iXBIYSMi7wOJuKGe9wDjgBnAVKAVsAO4SVWr/ICPItIfmAesxtt+/Sjuuka1Kg8Ricdd1IzGfTGcqqoTRKQd7ht3Y2A5cLuqnoxcpOHjaZ56SFWvrI7l4PnM0z2rNYD/qOpfRKQJQf5/VPqkYYwxJnwqe/OUMcaYMLKkYYwxJmCWNIwxxgTMkoYxxpiAWdIwxhgTMEsaxlRAIpIoIioiXSIdizG+LGkYY4wJmCUNY4wxAbOkYYwPEekvIt+KSKaI7BeRyT7DL4z0NBn1EpF5InJcRDaKyHV+jvN/noltTnomCPutn23iReQzETkkIhmeiZMuK7BZUxGZ5nl9q4j8upw+ujEBsaRhjIeI9MNNRPMTbmyi3wBXAP8usOmHuDF6rscNWzJNRLr6HOce4F+4cX2uAqYBz4nPzJIich6wAGgG3AtchxvmwXcAToDJwErP68nAyyLSu+yf1pjSsWFEjPEQkXlAtqoO8nnuElwiuQDoiUsgj6nq057Xo4C1wApVHeFZ3wnMVtVRPsd5BbgNN3/BCc+4YQOA9qp63E8sibiB9Z5S1Sc9z8XgRmR9U1WrwwyEpgKymoYxgIjUAS4EpopIjdwFmA9kAT18Ns8d+A1VzcHVOnK//bcEmuNqF74+BBrgkg+4Ybo/9JcwCpjt815ZwCbPexgTEZY0jHEa4UaFfQWXJHKXk7jpUn2bjQrOObAX18yEz889BbbJXW/s+dkE2B1AXIcKrJ8CagWwnzHlorLP3GdMqBzCzeA2HvjCz+tpuDkIwE2Jud/ntTPxJoDdPs/5OsvzM3fY6f14E4wxlYbVNIwBVPUYsAjoqKpL/Sy+s7vl3S3luYZxDfC956lUXIK5qcBb3AwcwV04B3ed5GYRsVqDqVSspmGM1x+Ar0UkB/gIOIqbnGY48JjPdneLyCnczGf3AOcCPwd3jUNExgOvi8h+YA5wMTAGeFRVT3iO8SfczJNzReQ5XM2jG7BfVd8q109pTBlYTcMYD1WdDwwE4oD3gM9wiWQn+a9RjMDVNmYAXYFbVHW5z3EmAw94tvkcl1B+r6p/9dlmA9Af2Ae8gbu4fiOwvZw+njEhYbfcGhMgERmJu+W2vqpmRDgcYyLCahrGGGMCZknDGGNMwKx5yhhjTMCspmGMMSZgljSMMcYEzJKGMcaYgFnSMMYYEzBLGsYYYwL2/wE/7CZZl9xbwQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the minibatch, full training set, and validation costs\n",
    "minibatch_x_inds = np.linspace(0, nb_of_iterations, num=nb_of_iterations*nb_of_batches)\n",
    "iteration_x_inds = np.linspace(1, nb_of_iterations, num=nb_of_iterations)\n",
    "# Plot the cost over the iterations\n",
    "# plt.plot(minibatch_x_inds, minibatch_costs, 'k-', linewidth=0.5, label='cost minibatches')\n",
    "plt.plot(iteration_x_inds, training_costs, 'b-', linewidth=3, label='training set')\n",
    "plt.plot(iteration_x_inds, validation_costs, 'r--', linewidth=3, label='validation set')\n",
    "# Add labels to the plot\n",
    "plt.xlabel('epoch', fontsize=15)\n",
    "plt.ylabel('loss', fontsize=15)\n",
    "# plt.title('Decrease of cost over backprop iteration')\n",
    "plt.legend()\n",
    "x1,x2,y1,y2 = plt.axis()\n",
    "plt.axis((0,nb_of_iterations,0,2.5))\n",
    "plt.grid()\n",
    "# plt.savefig(\"C:/Users/kzw/Desktop/loss.png\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the validation set is 0.94\n"
     ]
    }
   ],
   "source": [
    "# Get results of test data\n",
    "y_true = np.argmax(T_validation, axis=1)  \n",
    "activations = forward_step2(X_validation, layers)  \n",
    "y_pred = np.argmax(activations[-1], axis=1)  \n",
    "val_accuracy = metrics.accuracy_score(y_true, y_pred)  \n",
    "print('The accuracy on the validation set is {:.2f}'.format(val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set is 0.97\n"
     ]
    }
   ],
   "source": [
    "# Get results of test data\n",
    "y_true = np.argmax(T_test, axis=1)  # Get the target outputs\n",
    "activations = forward_step2(X_test, layers)  # Get activation of test samples\n",
    "y_pred = np.argmax(activations[-1], axis=1)  # Get the predictions made by the network\n",
    "test_accuracy = metrics.accuracy_score(y_true, y_pred)  # Test set accuracy\n",
    "print('The accuracy on the test set is {:.2f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigm : 2\n",
      "epoches : 50\n",
      "The accuracy on the validation set is 0.94\n",
      "The accuracy on the test set is 0.97\n"
     ]
    }
   ],
   "source": [
    "print \"sigm : \" + str(sigm)\n",
    "print(\"epoches : \" + str(nb_of_iterations)) \n",
    "print('The accuracy on the validation set is {:.2f}'.format(val_accuracy))\n",
    "print('The accuracy on the test set is {:.2f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confusion table\n",
    "conf_matrix = metrics.confusion_matrix(y_true, y_pred, labels=None)  # Get confustion matrix\n",
    "# Plot the confusion table\n",
    "class_names = ['${:d}$'.format(x) for x in range(0, 10)]  # Digit class names\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "# Show class labels on each axis\n",
    "ax.xaxis.tick_top()\n",
    "major_ticks = range(0,10)\n",
    "minor_ticks = [x + 0.5 for x in range(0, 10)]\n",
    "ax.xaxis.set_ticks(major_ticks, minor=False)\n",
    "ax.yaxis.set_ticks(major_ticks, minor=False)\n",
    "ax.xaxis.set_ticks(minor_ticks, minor=True)\n",
    "ax.yaxis.set_ticks(minor_ticks, minor=True)\n",
    "ax.xaxis.set_ticklabels(class_names, minor=False, fontsize=15)\n",
    "ax.yaxis.set_ticklabels(class_names, minor=False, fontsize=15)\n",
    "# Set plot labels\n",
    "ax.yaxis.set_label_position(\"right\")\n",
    "ax.set_xlabel('Predicted label')\n",
    "ax.set_ylabel('True label')\n",
    "fig.suptitle('Confusion table', y=1.03, fontsize=15)\n",
    "# Show a grid to seperate digits\n",
    "ax.grid(b=True, which=u'minor')\n",
    "# Color each grid cell according to the number classes predicted\n",
    "ax.imshow(conf_matrix, interpolation='nearest', cmap='binary')\n",
    "# Show the number of samples in each cell\n",
    "for x in xrange(conf_matrix.shape[0]):\n",
    "    for y in xrange(conf_matrix.shape[1]):\n",
    "        color = 'w' if x == y else 'k'\n",
    "        ax.text(x, y, conf_matrix[y,x], ha=\"center\", va=\"center\", color=color)       \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
