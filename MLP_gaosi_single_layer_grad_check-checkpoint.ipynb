{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import numpy as np # Matrix and vector computation package\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib inline\n",
    "# Set the seed of the numpy random number generator so that the tutorial is reproducable\n",
    "np.random.seed(seed=1)\n",
    "from sklearn import datasets, cross_validation, metrics # data and evaluation utils\n",
    "from matplotlib.colors import colorConverter, ListedColormap # some plotting functions\n",
    "import itertools\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "\n",
    "T = np.zeros((digits.target.shape[0],10))\n",
    "T[np.arange(len(T)), digits.target] += 1\n",
    "\n",
    "X_train, X_test, T_train, T_test = cross_validation.train_test_split(\n",
    "    digits.data, T, test_size=0.4)\n",
    "X_validation, X_test, T_validation, T_test = cross_validation.train_test_split(\n",
    "    X_test, T_test, test_size=0.5)\n",
    "# print(digits.target.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x100 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 1), dpi=100)\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(1,10,i+1)\n",
    "    ax.matshow(digits.images[i], cmap='binary') \n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "#给所有样本头部插入1\n",
    "def array_add_data2(activ):\n",
    "    \"\"\"\n",
    "        参数activ : 样本\n",
    "        作用 : 在数组头部插入数值1\n",
    "    \"\"\"\n",
    "    r = []\n",
    "    ret1 = np.append([1],activ)\n",
    "    r.append(ret1)\n",
    "    ret2 = np.array(r)\n",
    "    return ret2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9L,)\n",
      "(1L, 9L)\n",
      "[[1 2 3 4 5 6 7 8 9]]\n",
      "[[2.12078996e-04 5.76490482e-04 1.56706360e-03 4.25972051e-03\n",
      "  1.15791209e-02 3.14753138e-02 8.55587737e-02 2.32572860e-01\n",
      "  6.32198578e-01]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Define the non-linear functions used\n",
    "def logistic(z): \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# def logistic_deriv(y):  # Derivative of logistic function\n",
    "#     return np.multiply(y, (1 - y))\n",
    "    \n",
    "def softmax(z): \n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "\n",
    "d = np.array([1,2,3,4,5,6,7,8,9])\n",
    "d2 = np.expand_dims(d, axis=0)\n",
    "print(d.shape)\n",
    "print(d2.shape)\n",
    "# print(softmax(d))\n",
    "print(d2)\n",
    "print(softmax(d2))\n",
    "print(softmax(d2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layers used in this model\n",
    "class Layer(object):\n",
    "    \n",
    "#     def get_params_iter(self):\n",
    "#         return []\n",
    "    \n",
    "#     def get_params_grad(self, X, output_grad):\n",
    "#         return [],[]\n",
    "    \n",
    "    # def add高斯noise *****\n",
    "#     def add_gaosi_noise(self):\n",
    "#         pass\n",
    "    \n",
    "    # def remove高斯noise *****\n",
    "#     def remove_gaosi_noise(self):\n",
    "#         pass\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        pass\n",
    "    \n",
    "#     def get_input_grad(self, Y, output_grad=None, T=None):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(Layer):\n",
    "    \n",
    "    def __init__(self, n_in, n_out):\n",
    "        self.W = np.random.randn(n_in, n_out) * 0.1\n",
    "        self.b = np.zeros(n_out)\n",
    "        # 存下本层的结点数*****\n",
    "#         self.nodenum = n_out\n",
    "#         self.gaosi_noise = 0\n",
    "#         self.curbs = 0\n",
    "        \n",
    "    # def add高斯noise *****\n",
    "#     def add_gaosi_noise(self):\n",
    "#         #self.gaosi_noise = np.random.randn(self.curbs, self.nodenum) * 1\n",
    "#         self.gaosi_noise = 0\n",
    "        \n",
    "    # def remove高斯noise *****\n",
    "#     def remove_gaosi_noise(self):\n",
    "#         self.gaosi_noise = 0\n",
    "        \n",
    "#     def get_params_iter(self):\n",
    "#         return itertools.chain(np.nditer(self.W, op_flags=['readwrite']),\n",
    "#                                np.nditer(self.b, op_flags=['readwrite']))\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        return X.dot(self.W) + self.gaosi_noise + self.b\n",
    "        \n",
    "#     def get_params_grad(self, X, output_grad):\n",
    "#         JW = X.T.dot(output_grad)\n",
    "#         Jb = np.sum(output_grad, axis=0)\n",
    "#         return [g for g in itertools.chain(np.nditer(JW), np.nditer(Jb))],JW\n",
    "    \n",
    "#     def get_input_grad(self, Y, output_grad):\n",
    "#         return output_grad.dot(self.W.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = np.array([[1,1,1,1,1],[2,2,2,2,2],[3,3,3,3,3],[4,4,4,4,4],[5,5,5,5,5]])\n",
    "# print(d)\n",
    "# d[0:1,:] = 0\n",
    "# print(\"-------------\")\n",
    "# print(d)\n",
    "# # print(\"----------------------------------------\")\n",
    "# # w = np.random.randn(20, 10) * 0.1\n",
    "# # print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticLayer(Layer):\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        return logistic(X)\n",
    "    \n",
    "#     def get_input_grad(self, Y, output_grad):\n",
    "#         \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "#         return np.multiply(logistic_deriv(Y), output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxOutputLayer(Layer):\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        return softmax(X)\n",
    "    \n",
    "#     def get_input_grad(self, Y, T):\n",
    "#         re = (Y - T) / 1                                           #Y.shape[0]\n",
    "# #         print(\"--------- input_grad ----------\")\n",
    "# #         print(\"T.shape : \" + str(T.shape))\n",
    "# #         print(\"Y.shape : \" + str(Y.shape))\n",
    "# #         print(\"Y.shape[0] : \" + str(Y.shape[0]))\n",
    "#         return re\n",
    "    \n",
    "    def get_cost(self, Y, T):\n",
    "        multi = np.multiply(T, np.log(Y))\n",
    "        \n",
    "#         print(\"--------- cost ----------\")\n",
    "#         print(\"T.shape : \" + str(T.shape))\n",
    "#         print(\"Y.shape : \" + str(Y.shape))\n",
    "#         print(\"Y.shape[0] : \" + str(Y.shape[0]))\n",
    "        \n",
    "        return - multi.sum() / Y.shape[0] #Y.shape[0]                                                             大更改  单样本时应该除以1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "# 定义模型\n",
    "hidden_neurons_1 = 20  \n",
    "hidden_neurons_2 = 20 \n",
    "# Create the model\n",
    "layers = [] \n",
    "layers.append(LinearLayer(X_train.shape[1], hidden_neurons_1)) # (64L,20L)\n",
    "layers.append(LogisticLayer())\n",
    "# Add second hidden layer\n",
    "# layers.append(LinearLayer(hidden_neurons_1, hidden_neurons_2))\n",
    "# layers.append(LogisticLayer())\n",
    "layers.append(LinearLayer(hidden_neurons_2, T_train.shape[1])) #(20L,10L)\n",
    "layers.append(SoftmaxOutputLayer())\n",
    "\n",
    "print(T_train.shape[1])\n",
    "print(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_step(input_samples, layers,bs,noise1):\n",
    "    activations = [input_samples] \n",
    "    X = input_samples\n",
    "    for index in range(len(layers)): \n",
    "        layer = layers[index]\n",
    "        #layer.curbs = bs\n",
    "        #layer.add_gaosi_noise()\n",
    "        layer.gaosi_noise = noise1[index]\n",
    "        Y = layer.get_output(X)  \n",
    "        activations.append(Y)  \n",
    "        X = activations[-1]  \n",
    "    return activations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 前向传播 : add 高斯nosie *****\n",
    "# def forward_step2(input_samples, layers):\n",
    "#     activations = [input_samples] # List of layer activations\n",
    "#     X = input_samples\n",
    "#     for layer in layers:\n",
    "#         # *****remove\n",
    "#         layer.remove_gaosi_noise()\n",
    "#         Y = layer.get_output(X) \n",
    "#         activations.append(Y) \n",
    "#         X = activations[-1]  \n",
    "#     return activations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the backward propagation step as a method\n",
    "# def backward_step(activations, targets, layers):\n",
    "#     param_grads = collections.deque() \n",
    "#     jws = collections.deque() #***** get每一层的JW\n",
    "#     output_grad = None \n",
    "#     for layer in reversed(layers):   \n",
    "#         Y = activations.pop() \n",
    "#         if output_grad is None:\n",
    "#             input_grad = layer.get_input_grad(Y, targets)\n",
    "#         else: \n",
    "#             input_grad = layer.get_input_grad(Y, output_grad) # 输出a + 输出导数 -> 得输入导数  linear层时Y没有用到\n",
    "#         X = activations[-1]\n",
    "#         grads,JW = layer.get_params_grad(X, output_grad)      # 输入a + 输出导数 -> 此层的grad\n",
    "#         param_grads.appendleft(grads)\n",
    "#         output_grad = input_grad\n",
    "#         jws.appendleft(JW) #get本层的JW\n",
    "#     return list(param_grads),jws "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform gradient checking\n",
    "# nb_samples_gradientcheck = 10 # Test the gradients on a subset of the data\n",
    "# X_temp = X_train[0:nb_samples_gradientcheck,:]\n",
    "# T_temp = T_train[0:nb_samples_gradientcheck,:]\n",
    "# # Get the parameter gradients with backpropagation\n",
    "# activations = forward_step(X_temp, layers)\n",
    "# param_grads = backward_step(activations, T_temp, layers)\n",
    "\n",
    "# # Set the small change to compute the numerical gradient\n",
    "# eps = 0.0001\n",
    "# # Compute the numerical gradients of the parameters in all layers.\n",
    "# for idx in range(len(layers)):\n",
    "#     layer = layers[idx]\n",
    "#     layer_backprop_grads = param_grads[idx]\n",
    "#     # Compute the numerical gradient for each parameter in the layer\n",
    "#     for p_idx, param in enumerate(layer.get_params_iter()):\n",
    "#         grad_backprop = layer_backprop_grads[p_idx]\n",
    "#         # + eps\n",
    "#         param += eps\n",
    "#         plus_cost = layers[-1].get_cost(forward_step(X_temp, layers)[-1], T_temp)\n",
    "#         # - eps\n",
    "#         param -= 2 * eps\n",
    "#         min_cost = layers[-1].get_cost(forward_step(X_temp, layers)[-1], T_temp)\n",
    "#         # reset param value\n",
    "#         param += eps\n",
    "#         # calculate numerical gradient\n",
    "#         grad_num = (plus_cost - min_cost)/(2*eps)\n",
    "#         # Raise error if the numerical grade is not close to the backprop gradient\n",
    "#         if not np.isclose(grad_num, grad_backprop):\n",
    "#             raise ValueError('Numerical gradient of {:.6f} is not close to the backpropagation gradient of {:.6f}!'.format(float(grad_num), float(grad_backprop)))\n",
    "# print('No gradient errors found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the minibatches\n",
    "# batch_size = 25  # Approximately 25 samples per batch\n",
    "# nb_of_batches = X_train.shape[0] / batch_size  # Number of batches\n",
    "# # Create batches (X,Y) from the training set\n",
    "# XT_batches = zip(\n",
    "#     np.array_split(X_train, nb_of_batches, axis=0),  # X samples\n",
    "#     np.array_split(T_train, nb_of_batches, axis=0))  # Y targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a method to update the parameters\n",
    "# def update_params(layers, param_grads, learning_rate):\n",
    "#     \"\"\"\n",
    "#     Function to update the parameters of the given layers with the given gradients\n",
    "#     by gradient descent with the given learning rate.\n",
    "#     \"\"\"\n",
    "#     for layer, layer_backprop_grads in zip(layers, param_grads):\n",
    "#         for param, grad in itertools.izip(layer.get_params_iter(), layer_backprop_grads):\n",
    "#             # The parameter returned by the iterator point to the memory space of\n",
    "#             #  the original layer and can thus be modified inplace.\n",
    "#             param -= learning_rate * grad  # Update each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法:给某node的某权重加微小数值\n",
    "def change_weight(layers,layer_index,node_index,weight_index,eps):\n",
    "    #print(\"eps : \" + str(eps))\n",
    "    layer = layers[layer_index - 1]\n",
    "    #print(\"前w : \" + str(layer.W[weight_index - 1][node_index - 1]))\n",
    "    layer.W[weight_index - 1][node_index - 1] += eps\n",
    "    #print(\"后w : \" + str(layer.W[weight_index - 1][node_index - 1]))\n",
    "    #print(\"-----------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "均值     : -0.738197245051427\n",
      "标准误差 : 0.001582725558380\n"
     ]
    }
   ],
   "source": [
    "# Perform backpropagation\n",
    "# initalize some lists to store the cost for future analysis        \n",
    "# minibatch_costs = []\n",
    "# training_costs = []\n",
    "# validation_costs = []\n",
    "\n",
    "#取单张图片\n",
    "x1 = digits.data[0]\n",
    "x1 = np.expand_dims(x1, axis=0)\n",
    "t1 = T[0]\n",
    "t1 = np.expand_dims(t1, axis=0)\n",
    "# print(type(x1))\n",
    "# print(x1.shape)\n",
    "# print(x1)\n",
    "# print(\"------------------------------------------------------\")\n",
    "# print(type(t1))\n",
    "# print(t1.shape)\n",
    "# print(t1)\n",
    "\n",
    "eps = 0.000000001\n",
    "sigm = 5 #35\n",
    "max_nb_of_iterations = 50000  \n",
    "# 存下每次迭代时此节点某个权重的grad\n",
    "w = []\n",
    "layer_index,node_index,weight_index= 3,1,1  #layer : 1,3\n",
    "\n",
    "#  开始训练\n",
    "for iteration in range(max_nb_of_iterations):\n",
    "    bs = 1\n",
    "    \n",
    "    # 准备好各layer的高斯noise\n",
    "    noise1 = []\n",
    "    noise1.append(np.random.randn(1,20)*sigm)\n",
    "    noise1.append([])\n",
    "    noise1.append(np.random.randn(1,10)*sigm)\n",
    "    noise1.append([])\n",
    "    \n",
    "    # 改变w的值, + eps\n",
    "    change_weight(layers,layer_index,node_index,weight_index,eps)\n",
    "    activations1 = forward_step(x1,layers,bs,noise1)\n",
    "#     print(\"--------- ac ---------\")\n",
    "#     print(activations1[-1].shape)\n",
    "    lossa = layers[-1].get_cost(activations1[-1],t1)\n",
    "    # 恢复w的值\n",
    "    change_weight(layers,layer_index,node_index,weight_index,-1*eps)\n",
    "    \n",
    "    # 改变w的值, - eps\n",
    "    change_weight(layers,layer_index,node_index,weight_index,-1*eps)\n",
    "    activations2 = forward_step(x1,layers,bs,noise1)\n",
    "    lossb = layers[-1].get_cost(activations2[-1],t1)\n",
    "    # 恢复w的值\n",
    "    change_weight(layers,layer_index,node_index,weight_index,eps)\n",
    "    grad = (lossa - lossb)/(2*eps)\n",
    "    w.append(grad)\n",
    "    \n",
    "    # 显示迭代次数\n",
    "    if (iteration + 1) % 10000 == 0:\n",
    "        print(iteration + 1) \n",
    "# 显示w的方差和均值\n",
    "print('均值     : '+ str(np.mean(w)))\n",
    "print('标准误差 : '+ \"{:.15f}\".format( np.std(w)/np.math.sqrt(max_nb_of_iterations) )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-439-6d05ec419670>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-439-6d05ec419670>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    0.001590605978503    0.001590195493139    0.001593100262609    0.001590821526654    0.001582926987921    0.001597496964117\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "-0.7381045555059148  -0.7373082539131488  -0.7359272784028735  -0.7374334871762546  -0.7375924447874685  -0.7338842603461931\n",
    " 0.001590605978503    0.001590195493139    0.001593100262609    0.001590821526654    0.001582926987921    0.001597496964117\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[10]\n",
      " [ 9]\n",
      " [ 8]\n",
      " [ 7]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 1]]\n",
      "[10.]\n",
      "[[10.]]\n",
      "(1L,)\n",
      "(1L, 1L)\n"
     ]
    }
   ],
   "source": [
    "#(1L,10L)和(10L,)进行矩阵运算时数值是一致的\n",
    "# x1 = digits.data[0]\n",
    "# x2 = np.expand_dims(x1, axis=0)\n",
    "t1 = T[0]\n",
    "t2 = np.expand_dims(t1, axis=0)\n",
    "t3 = np.array([[10],[9],[8],[7],[6],[5],[4],[3],[2],[1]])\n",
    "# print(t1)\n",
    "# print(t2)\n",
    "# print(t3)\n",
    "# print(t1.dot(t3))\n",
    "# print(t2.dot(t3))\n",
    "# print(t1.dot(t3).shape)\n",
    "# print(t2.dot(t3).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1L, 65L)\n",
    "<type 'numpy.ndarray'>\n",
    "[[ 1.  0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.\n",
    "   3. 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.\n",
    "   0.  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10.\n",
    "  12.  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]]\n",
    "---------------------------------------------------------------\n",
    "(1L, 10L)\n",
    "<type 'numpy.ndarray'>\n",
    "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BP grad check :    \n",
    "sigma = 1   index: 2,3,6  eps = 0.000000001  iteration=50000    置信区间\n",
    "mean : 0.0020495769240125306  0.002078444063247087  0.0020799768912782564  0.002117528955514203 0.0020741216685826735  0.002076547270246642\n",
    "SE   : 0.000016759583866      0.000017834399265     0.000018665902527      0.000017771795210    0.000017511329352      0.000016863149006\n",
    "\n",
    "mean : 0.0020779067305687353  0.00208111487431939  0.0020924696661484     0.0020750405832492547 0.0020818453305704305  0.002090918322233959\n",
    "SE   : 0.000017978795028      0.000016744602387    0.000017283383773      0.000016808492089     0.000017181462061      0.000017380478599"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "----------\n",
    "BP grad check :    \n",
    "sigma = 1   index: 1,2,5  eps = 0.000000001  iteration=50000    基本都在置信区间内 个别的不在置信区间内\n",
    "0.00023423955386459028 0.000231929280294918  0.00023035868101928259  0.0002506730734097573  0.0002880321897658078  0.0002673656046559003\n",
    "0.000023695903378      0.000023736698396     0.000023740645944       0.000023825522032      0.000024117845601      0.000024149742318\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "----------\n",
    "BP grad check :    \n",
    "sigma = 1   index: 3,1,1 eps = 0.000000001  基本都在置信区间内 个别的不在置信区间内\n",
    "0.010026767945925386  0.00991859680651852    0.00998332517238021     0.010114318405862743   0.009963580570038209   0.010053343907773238  \n",
    "0.000073841999104     0.000070439197762      0.000072341215463       0.000072881063699      0.000070908132449      0.000072668180733\n",
    "\n",
    "0.009945580512760177  0.00992164006702323    0.009982833872879128    0.009892310247339164   0.009912317212867892   0.009858059007300524    \n",
    "0.000071999121371     0.000071668311052      0.000071485047325       0.000070741112615      0.000070940630232      0.000069826406757 \n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00206633650788\n",
      "0.00206060966398\n"
     ]
    }
   ],
   "source": [
    "# 验证置信区间\n",
    "d1 =   0.0020495769240125306\n",
    "eps1 = 0.000016759583866\n",
    "d2 =   0.002078444063247087\n",
    "eps2 = 0.000017834399265 \n",
    "print(d1 + eps1) #大\n",
    "print(d2 - eps2) #小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "----------\n",
    "BP grad check :    \n",
    "sigma = 100   index: 3,1,1 eps = 0.000000001  置信区间  nan问题\n",
    "0.05077270555556837  0.049058228592304694    0.04964256096726661     0.050799105929777516   0.04784441040674067*   0.04956721125636716  \n",
    "0.000968773730964    0.000954082790953       0.000959551088207       0.000970190460007      0.000942596747268      0.000957921011789\n",
    "\n",
    "0.04802735342241512  0.04929215473875475     0.04935728961586507     0.04942941053883725    0.04974981535710003    0.050218366790931876    \n",
    "0.000944178856878*   0.000956777269162       0.000957426074984       0.000958272294223      0.000960622189282      0.000964762511954  \n",
    "----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选定的权重 : layer2,node1,weight1\n",
    "============================================\n",
    "BP法一 :(正常方法计算grad) :           sigma = 1                        全在置信区间\n",
    "mean : 0.010026768487083235  0.00991859750727568    0.009983325531141016    0.01011431937186198    0.009947821015864488    0.009963580580966535\n",
    "SE   : 0.000073841998315     0.000070439199795      0.000072341219021       0.000072881065431      0.000071686513809       0.000070908133642 \n",
    "\n",
    "BP法二 :(梯度的数值逼近计算grad) :     sigma = 1     eps = 0.000000001  全在置信区间        \n",
    "mean : 0.010026589128686593  0.009918825861765512   0.009983802152502273    0.010113635804803735   0.00994806789736402     0.009963361458364961\n",
    "SE   : 0.000073842244197     0.000070438961814      0.000072343052882       0.000072879454182      0.000071686364345       0.000070908224526\n",
    "\n",
    "------------------------------------------\n",
    "# 选定的权重 : layer2,node1,weight1  \n",
    "新算法法一(论文方法算grad) :        sigma = 1   index: 3,1,1                     基本不在置信区间里\n",
    "mean : -1.0399425268865687   -0.9582785825537358    -0.9238959524971541    -1.0109073955813968   -0.9219608197997469   -0.8530126228738921\n",
    "SE   :  0.001779915596119     0.001756311087796      0.001731736375919      0.001763361357926     0.001733751541406     0.001687950497858\n",
    "\n",
    "新算法法二(梯度的数值逼近计算grad) :sigma = 1   index: 3,1,1  eps = 0.000000001  都不在置信区间\n",
    "mean : 0.01520875838916663    0.016980545326046245   0.019024104253098643    0.01668341347920224   0.021800709326530446   0.01403939312583091\n",
    "SE   : 0.000051951133106      0.000056440500778      0.000060850712043       0.000055290239140     0.000066330304731      0.000049409087263\n",
    "============================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the minibatch, full training set, and validation costs\n",
    "minibatch_x_inds = np.linspace(0, nb_of_iterations, num=nb_of_iterations*nb_of_batches)\n",
    "iteration_x_inds = np.linspace(1, nb_of_iterations, num=nb_of_iterations)\n",
    "# Plot the cost over the iterations\n",
    "plt.plot(minibatch_x_inds, minibatch_costs, 'k-', linewidth=0.5, label='cost minibatches')\n",
    "plt.plot(iteration_x_inds, training_costs, 'r-', linewidth=2, label='cost full training set')\n",
    "plt.plot(iteration_x_inds, validation_costs, 'b-', linewidth=3, label='cost validation set')\n",
    "# Add labels to the plot\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('$\\\\xi$', fontsize=15)\n",
    "plt.title('Decrease of cost over backprop iteration')\n",
    "plt.legend()\n",
    "x1,x2,y1,y2 = plt.axis()\n",
    "plt.axis((0,nb_of_iterations,0,2.5))\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results of test data\n",
    "y_true = np.argmax(T_test, axis=1)  # Get the target outputs\n",
    "activations = forward_step(X_test, layers)  # Get activation of test samples\n",
    "y_pred = np.argmax(activations[-1], axis=1)  # Get the predictions made by the network\n",
    "test_accuracy = metrics.accuracy_score(y_true, y_pred)  # Test set accuracy\n",
    "print('The accuracy on the test set is {:.2f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f75c37aae90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show confusion table\n",
    "conf_matrix = metrics.confusion_matrix(y_true, y_pred, labels=None)  # Get confustion matrix\n",
    "# Plot the confusion table\n",
    "class_names = ['${:d}$'.format(x) for x in range(0, 10)]  # Digit class names\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "# Show class labels on each axis\n",
    "ax.xaxis.tick_top()\n",
    "major_ticks = range(0,10)\n",
    "minor_ticks = [x + 0.5 for x in range(0, 10)]\n",
    "ax.xaxis.set_ticks(major_ticks, minor=False)\n",
    "ax.yaxis.set_ticks(major_ticks, minor=False)\n",
    "ax.xaxis.set_ticks(minor_ticks, minor=True)\n",
    "ax.yaxis.set_ticks(minor_ticks, minor=True)\n",
    "ax.xaxis.set_ticklabels(class_names, minor=False, fontsize=15)\n",
    "ax.yaxis.set_ticklabels(class_names, minor=False, fontsize=15)\n",
    "# Set plot labels\n",
    "ax.yaxis.set_label_position(\"right\")\n",
    "ax.set_xlabel('Predicted label')\n",
    "ax.set_ylabel('True label')\n",
    "fig.suptitle('Confusion table', y=1.03, fontsize=15)\n",
    "# Show a grid to seperate digits\n",
    "ax.grid(b=True, which=u'minor')\n",
    "# Color each grid cell according to the number classes predicted\n",
    "ax.imshow(conf_matrix, interpolation='nearest', cmap='binary')\n",
    "# Show the number of samples in each cell\n",
    "for x in xrange(conf_matrix.shape[0]):\n",
    "    for y in xrange(conf_matrix.shape[1]):\n",
    "        color = 'w' if x == y else 'k'\n",
    "        ax.text(x, y, conf_matrix[y,x], ha=\"center\", va=\"center\", color=color)       \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
