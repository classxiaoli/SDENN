{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 介绍 : 新算法,不加weibull_noise,所有节点加高斯noise,单个样本循环\n",
    "    #grad计算方法 : grad = ( a*noise*loss )/sigm**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import numpy as np # Matrix and vector computation package\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib inline\n",
    "# Set the seed of the numpy random number generator so that the tutorial is reproducable\n",
    "np.random.seed(seed=1)\n",
    "from sklearn import datasets, cross_validation, metrics # data and evaluation utils \n",
    "from matplotlib.colors import colorConverter, ListedColormap # some plotting functions\n",
    "import itertools\n",
    "import collections\n",
    "from scipy import stats\n",
    "from scipy.stats import weibull_min\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#给逻辑层的输出的头部插入1\n",
    "def array_add_data(activ):\n",
    "    \"\"\"\n",
    "    参数activ : 每层的输出(激活)  第一层 : (32L, 20L)\n",
    "    作用 : 在数组头部插入数值1\n",
    "    \"\"\"\n",
    "    r = []\n",
    "    ret1 = 0\n",
    "    for i in range(activ.shape[0]):\n",
    "        ret1 = np.array(np.append([1],activ[i])) #list\n",
    "        r.append(ret1)\n",
    "    r = np.array(r) # 类型转换:list -> array\n",
    "    return r\n",
    "\n",
    "#给所有样本头部插入1\n",
    "def array_add_data2(activ):\n",
    "    \"\"\"\n",
    "        参数activ : 样本\n",
    "        作用 : 在数组头部插入数值1\n",
    "    \"\"\"\n",
    "    r = []\n",
    "    ret1 = np.append([1],activ)\n",
    "    r.append(ret1)\n",
    "    ret2 = np.array(r)\n",
    "    return ret2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the non-linear functions used\n",
    "def logistic(z): \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# def logistic_deriv(y):  # Derivative of logistic function\n",
    "#     return np.multiply(y, (1 - y))\n",
    "    \n",
    "def softmax(z): \n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layers used in this model\n",
    "class Layer(object):\n",
    "    \n",
    "#     def get_params_iter(self):\n",
    "#         return []\n",
    "    \n",
    "#     def get_params_grad(self, X, output_grad):\n",
    "#         return []\n",
    "      \n",
    "#     def get_params_update(self, X, loss):\n",
    "#         return []\n",
    "    \n",
    "#     def add_noise(self):\n",
    "#         pass\n",
    "    \n",
    "#     def remove_noise(self):\n",
    "#         pass\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        pass\n",
    "    \n",
    "#     def get_input_grad(self, Y, output_grad=None, T=None):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200L, 1000L)\n",
      "[[ 1.62434536 -0.61175641 -0.52817175 ... -0.06962454  0.35387043\n",
      "  -0.18695502]\n",
      " [-0.15323616 -2.43250851  0.50798434 ... -0.92165905  0.64737512\n",
      "   1.38682559]\n",
      " [ 0.48951662  0.23879586 -0.44811181 ... -0.15065961 -1.40002289\n",
      "  -1.30106608]\n",
      " ...\n",
      " [ 0.46196742  0.26019753  0.63998131 ... -1.91183213  1.42065239\n",
      "   0.07642459]\n",
      " [ 0.02925599  0.66566426  1.22481623 ...  1.76967057  0.04718969\n",
      "   0.62738052]\n",
      " [-2.11132389  1.07358862  0.33936988 ...  0.83363722  0.00373374\n",
      "   0.77219844]]\n",
      "0.004468680118936303\n",
      "0.9976699065079347\n"
     ]
    }
   ],
   "source": [
    "d = np.random.randn(200,1000)\n",
    "print(d.shape)\n",
    "print(d)\n",
    "print(d.mean())\n",
    "print(d.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200L, 1000L)\n",
      "[[-0.78634044  2.1630398  -0.72936364 ...  0.87737166 -0.9166194\n",
      "   0.08977909]\n",
      " [-1.95889283  1.65138688 -0.93537478 ... -0.95639204 -0.43851766\n",
      "  -1.69272875]\n",
      " [-0.30917902  1.12606497  1.54834609 ... -0.96608512  0.91486773\n",
      "  -0.99338726]\n",
      " ...\n",
      " [-0.02297056  1.92534557  1.00620062 ... -0.84046465 -1.65403107\n",
      "   0.23506831]\n",
      " [ 1.2624318  -0.64760483 -0.13283537 ...  3.11889097 -0.22261589\n",
      "  -1.33357085]\n",
      " [-2.23425883  0.30777533 -0.21992508 ...  0.96315475 -0.51563458\n",
      "   0.60426468]]\n",
      "-0.0015192019451783648\n",
      "1.0015640203171052\n"
     ]
    }
   ],
   "source": [
    "d2 = np.random.normal(loc=0, scale=1, size=(200,1000))\n",
    "print(d2.shape)\n",
    "print(d2)\n",
    "print(d2.mean())\n",
    "print(d2.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class discreteLayer(Layer):\n",
    "    \"\"\"The linear layer performs a linear transformation to its input.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_in, n_out):\n",
    "        \"\"\" \n",
    "            对隐藏层参数进行初始化\n",
    "            n_in是输入变量的数量\n",
    "            n_out是输出变量的数量\n",
    "        \"\"\"\n",
    "#         self.sig=sig\n",
    "#         self.batch_size = batch_size\n",
    "#         self.node_sum = n_out\n",
    "        self.W = np.random.randn(n_in, n_out) * 0.1\n",
    "        self.W[0:1,:] = 0\n",
    "#         print(\"------------------- self.W -------------------\")\n",
    "#         print(self.W.shape)\n",
    "#         print(self.W)\n",
    "#         self.noise = 0\n",
    "    # training set 训练时: 添加高斯noise\n",
    "#     def add_noise(self): \n",
    "#         self.noise = 0\n",
    "    \n",
    "    # 移除选定结点的高斯noise\n",
    "#     def remove_targetnode_gaosi_noise(self,node_index):\n",
    "#         for n in self.noise:\n",
    "#             n[node_index - 1] = 0 \n",
    "    \n",
    "    # 移除高斯noise   (validation set 验证时)\n",
    "#     def remove_noise(self):\n",
    "#         self.noise = np.zeros(self.W.shape[1], dtype=float, order='C')\n",
    "    \n",
    "#     def get_params_iter(self):\n",
    "#         return self.W\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        # 线性变换\n",
    "        xw = X.dot(self.W)\n",
    "        \n",
    "#         print(\"=============== 线性out put ================\")\n",
    "#         print(xw)\n",
    "        re = xw + self.noise \n",
    "        return re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1]\n",
      " [2 2 2 2 2]\n",
      " [3 3 3 3 3]\n",
      " [4 4 4 4 4]\n",
      " [5 5 5 5 5]]\n",
      "-------------\n",
      "[[0 0 0 0 0]\n",
      " [2 2 2 2 2]\n",
      " [3 3 3 3 3]\n",
      " [4 4 4 4 4]\n",
      " [5 5 5 5 5]]\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "d = np.array([[1,1,1,1,1],[2,2,2,2,2],[3,3,3,3,3],[4,4,4,4,4],[5,5,5,5,5]])\n",
    "print(d)\n",
    "d[0:1,:] = 0\n",
    "print(\"-------------\")\n",
    "print(d)\n",
    "print(d.sum())\n",
    "# print(\"----------------------------------------\")\n",
    "# w = np.random.randn(20, 10) * 0.1\n",
    "# print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticLayer(Layer):\n",
    "    \"\"\"The logistic layer applies the logistic function to its inputs.\"\"\"\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         self.noise = 0\n",
    "#         self.b = 0\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        # 在头部插入数值1\n",
    "        logi = logistic(X)\n",
    "        Y = array_add_data(logi)\n",
    "        \n",
    "#         print(\"=============== 逻辑层 ================\")\n",
    "#         print(logi)\n",
    "#         print(\"--------------------------------------------------------------\")\n",
    "#         print(Y)\n",
    "        \n",
    "        return Y\n",
    "    \n",
    "#     def get_input_grad(self, Y, output_grad):\n",
    "#         return np.multiply(logistic_deriv(Y), output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxOutputLayer(Layer):\n",
    "    \"\"\" \n",
    "        输出层\n",
    "    \"\"\"  \n",
    "#     def __init__(self):\n",
    "#         self.noise = 0\n",
    "#         self.b = 0\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        r = softmax(X)\n",
    "#         print(\"-------------- softmax --------------\")\n",
    "#         print(X)\n",
    "#         print(\"----->\")\n",
    "#         print(r)\n",
    "        return r\n",
    "    \n",
    "    def get_cost(self, A, T):\n",
    "        \n",
    "#         print(\"--------- cost ----------\")\n",
    "#         print(A)\n",
    "#         print(T)\n",
    "#         print(\"T.shape : \" + str(T.shape))\n",
    "#         print(\"A.shape : \" + str(A.shape))\n",
    "#         print(\"A.shape[0] : \" + str(A.shape[0]))\n",
    "        \n",
    "        re = - np.multiply(T, np.log(A)).sum() / A.shape[0]      #A.shape[0]                         大更改  单样本时应该除以1\n",
    "        return re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_step(input_samples,layers,noise1):\n",
    "    activations = [input_samples] \n",
    "    X = input_samples\n",
    "    for index in range(len(layers)): \n",
    "        layer = layers[index]\n",
    "        layer.noise = noise1[index]\n",
    "        Y = layer.get_output(X)\n",
    "        activations.append(Y)  \n",
    "        X = activations[-1]  \n",
    "    return activations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#得到样本\n",
    "\n",
    "# 在所有样本的头部添加数值1\n",
    "digits = datasets.load_digits()\n",
    "d = digits.data # array\n",
    "# print(data.shape) # (1797L, 64L)\n",
    "target_list = [] # list\n",
    "target_data = []\n",
    "# single_array = np.array([1])\n",
    "for i in range(len(d)):\n",
    "    # 取出数据\n",
    "    a = d[i]\n",
    "    # 头部添加1 \n",
    "    a_temp = array_add_data2(a)[0]\n",
    "    # 添加到总的array中\n",
    "    target_list.append(a_temp)\n",
    "target_data = np.array(target_list) \n",
    "\n",
    "T = np.zeros((digits.target.shape[0],10))\n",
    "T[np.arange(len(T)), digits.target] += 1\n",
    "\n",
    "#有问题 : 这里的划分应该是随机的,和digits.data不对应\n",
    "X_train, X_test, T_train, T_test = cross_validation.train_test_split(\n",
    "    target_data, T, test_size=0.4)\n",
    "X_validation, X_test, T_validation, T_test = cross_validation.train_test_split(\n",
    "    X_test, T_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 创建minibatches\n",
    "# X_train = X_train[0:1024,:] #取1024方便计算\n",
    "# T_train = T_train[0:1024,:]\n",
    "batch_size = 1#    adjust\n",
    "# nb_of_batches = X_train.shape[0] / batch_size  # 32批\n",
    "# # Create batches (X,Y) from the training set\n",
    "# XT_batches = zip(\n",
    "#     np.array_split(X_train, nb_of_batches, axis=0),  \n",
    "#     np.array_split(T_train, nb_of_batches, axis=0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得grad\n",
    "def get_grad(ac,noise,sigm,loss):\n",
    "    grad = []\n",
    "    gra = 0\n",
    "    for i in range(len(noise)):\n",
    "        if i%2 == 0 :\n",
    "            a = ac[i]\n",
    "            noi = noise[i]\n",
    "            do  = a.T.dot(noi)\n",
    "            gra = ( do * loss ) / (sigm[ int(i/2) ]**2)  # (20L,65L)  (10L,20L)\n",
    "            grad.append(gra)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65L, 20L)\n",
      "(21L, 20L)\n"
     ]
    }
   ],
   "source": [
    "# 定义模型\n",
    "hidden_neurons_1 = 20  # 第一个隐藏层的神经元数目\n",
    "layers = [] #\n",
    "# 添加第一个隐层\n",
    "layers.append(discreteLayer(X_train.shape[1], hidden_neurons_1))  \n",
    "layers.append(LogisticLayer())\n",
    "# 添加第二个隐层\n",
    "layers.append(discreteLayer(21, hidden_neurons_1))  \n",
    "layers.append(LogisticLayer())\n",
    "# 添加输出层\n",
    "layers.append(discreteLayer(21, T_train.shape[1])) \n",
    "layers.append(SoftmaxOutputLayer())\n",
    "\n",
    "print(layers[0].W.shape)\n",
    "print(layers[2].W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import division\n",
    "# d = 1/3\n",
    "# print(round(d,10)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "-0.66571496 -0.91276122 0.3584759 1.60614175 -2.67369432 -1.85072804 \n",
      "1.09218967 1.09489627 1.09264788 1.09297561 1.09244639 1.09689047 \n",
      "\n",
      "-1.93424753 -1.68519939 -0.54053372 -1.09897671 1.26274579 1.29958365 \n",
      "1.10474532 1.10060991 1.09582594 1.09037563 1.10200778 1.1034387 \n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "means = []\n",
    "ses = []\n",
    "\n",
    "for i in range(12):\n",
    "    if i%2 ==1:\n",
    "        print i+1\n",
    "    #迭代训练 : 单张图片  指定结点 target_data[0]\n",
    "    x1 = target_data[0]\n",
    "    t1 = T[0]\n",
    "    x1 = np.expand_dims(x1, axis=0)\n",
    "    t1 = np.expand_dims(t1, axis=0)\n",
    "\n",
    "    # 开始训练\n",
    "    sigm = [2,0.02,0.01] \n",
    "    max_nb_of_iterations = 10000 # 训练次数\n",
    "    # 选定一个结点\n",
    "    layer_index,node_index,weight_index = 3,1,1   #layer : 1,2,3\n",
    "    w = []\n",
    "    for iteration in range(max_nb_of_iterations): \n",
    "        # 准备好各layer的高斯noise\n",
    "        noise1 = []\n",
    "        noise1.append(np.random.randn(1,20)*sigm[0])\n",
    "        noise1.append([])\n",
    "        noise1.append(np.random.randn(1,20)*sigm[1])\n",
    "        noise1.append([])\n",
    "        noise1.append(np.random.randn(1,10)*sigm[2])\n",
    "        noise1.append([])\n",
    "\n",
    "        activations = forward_step(x1,layers,noise1)\n",
    "        loss = layers[-1].get_cost(activations[-1],t1) # activations1[-1]二维数组: [[.....]]\n",
    "\n",
    "        grad = get_grad(activations,noise1,sigm,loss)      # grad1:(65L,20L)  grad2:(21L,10L)\n",
    "        w_grad = grad[layer_index - 1][weight_index][node_index - 1]\n",
    "        w.append(w_grad)\n",
    "        \n",
    "    m = round( (np.mean(w)) , 8) \n",
    "    s = round( (np.std(w)/math.sqrt(max_nb_of_iterations)) , 8)     \n",
    "    means.append(m)\n",
    "    ses.append(s)\n",
    "    \n",
    "m1=\"\"\n",
    "s1=\"\"\n",
    "m2=\"\"\n",
    "s2=\"\"\n",
    "for j in range(int(len(means)/2)):\n",
    "    m1 += str(means[j]) + \" \"\n",
    "print(m1)\n",
    "for j in range(int(len(ses)/2)):\n",
    "    s1 += str(ses[j]) + \" \"\n",
    "print(s1)\n",
    "print\n",
    "for j in range(int(len(means)/2)):\n",
    "    m2 += str(means[j + 6]) + \" \"\n",
    "print(m2)\n",
    "for j in range(int(len(ses)/2)):\n",
    "    s2 += str(ses[j + 6]) + \" \"\n",
    "print(s2)\n",
    "#     print('均值    : '+ str(np.mean(w)))\n",
    "#     print('标准误差 : '+ \"{:.15f}\".format( np.std(w)/math.sqrt(max_nb_of_iterations) )) \n",
    "#     print(\"----------------------------------\")\n",
    "#     if j == 5:\n",
    "#         print(\"==================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.\n",
      "  3. 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.\n",
      "  0.  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10.\n",
      " 12.  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(target_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer1_index :  1,16,60  1,1,11  1,3,30\n",
    "# layer2_index :  2,9 ,9   2,2,1  2,1,10\n",
    "# layer3_index :  3,8 ,10  3,5,5  3,1,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
